<article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://lonepatient.top/2018/04/20/Deep_Learning_For_Computer_Vision_With_Python_PB_10.html"><span hidden="" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="eamlife"><meta itemprop="description" content=""><meta itemprop="image" content="/images/touxiang.jpeg"></span><span hidden="" itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization"><meta itemprop="name" content="eamlife's blog"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">深度学习与计算机视觉(PB-10)-Kaggle之猫狗比赛</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-04-20T07:27:08+08:00">2018-04-19 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/深度学习/" itemprop="url" rel="index"><span itemprop="name">深度学习</span> </a></span>， <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/深度学习/计算机视觉/" itemprop="url" rel="index"><span itemprop="name">计算机视觉</span> </a></span></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/04/20/Deep_Learning_For_Computer_Vision_With_Python_PB_10.html#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/2018/04/20/Deep_Learning_For_Computer_Vision_With_Python_PB_10.html" itemprop="commentCount">0</span> </a></span><span id="/2018/04/20/Deep_Learning_For_Computer_Vision_With_Python_PB_10.html" class="leancloud_visitors" data-flag-title="深度学习与计算机视觉(PB-10)-Kaggle之猫狗比赛"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数:</span> <span class="leancloud-visitors-count">2</span></span><div class="post-wordcount"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计:</span> <span title="字数统计">8.3k 字</span></div></div></header><div id="copyBtn" style="opacity: 0; position: absolute;top:0px;display: none;line-height: 1; font-size:1.5em"><span id="imgCopy"><i class="fa fa-paste fa-fw"></i></span><span id="imgSuccess" style="display: none;"><i class="fa fa-check-circle fa-fw" aria-hidden="true"></i></span></div><div class="post-body" itemprop="articleBody"><p>在<a href="http://lonepatient.top/2018/04/09/Deep_Learning_For_Computer_Vision_With_Python_PB_09.html">第9节</a>中，我们提到了当数据太大无法加载到内存中时，如何使用HDF5保存大数据集——我们自定义了一个python脚本将原始图像数据集序列化为高效的HDF5数据集。在HDF5数据集中读取图像数据集可以避免I/O延迟问题，从而加快训练过程。</p><a id="more"></a><p>假设我们有N张保存在磁盘上的图像数据，之前的做法是定义了一个数据生成器，该生成器按顺序从磁盘中加载图像，N张图像共需要进行N个读取操作，每个图像一个读取操作，这样会存在I/O延迟问题。如果将图像数据集保存到HDF5数据集中，我们可以一次性读取batch大小的图像数据。这样极大地减少了I/O调用的次数，并且可以使用非常大的图像数据集。</p><p>在本节中，我们将学习如何为HDF5数据集定义一个图像生成器，从而方便使用Keras训练卷积神经网络。生成器会不断地从HDF5数据集中生成用于训练网络的数据和对应的标签，直到我们的模型达到足够低的损失/高精度才会停止。</p><p>在训练模型之前，我们将实现三种新的图像预处理方法——零均值化、patch Preprocessing和随机裁剪(也称为10-cropping或过采样)。之后，我们将利用Krizhevsky等人2012年的论文《ImageNet Classification with Deep Convolutional Neural Networks 》提出的AlexNet网络结构训练猫狗数据集，并在测试集上评估性能，另外，为了提高准确度，我们将对测试集使用过采样方法。在下面结果中，我们将看到使用AlexNet网络架构+裁剪方法可以获得比赛的top50。</p><h2 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h2><p>在本节中，我们将实现三个新的图像预处理方法:</p><ul><li>1.零均值化(数据归一化的一种形式)：将输入的图像减去数据集中的红色、绿色和蓝色三个颜色通道的平均值。</li><li>2.patch Preprocessing：在训练过程中，随机从原始图像中提取MxN大小特征图像。</li><li>3.过采样：在测试时，对输入图像的五个区域(四个角+中心区域)进行裁剪并且对剪裁之后的特征图像进行水平翻转，总共会得到10张特征图像。</li></ul><p>其中需要注意的是，过采样方法我们只在测试数据集上使用，通过过采样方法，我们得到10张特征图像，然后模型会对每一张图像进行预测，得到10个结果，最后通过投票或者平均计算得到最终的结果，从结果中我们将看到该方法有助于提高分类准确度。</p><h3 id="零均值"><a href="#零均值" class="headerlink" title="零均值"></a>零均值</h3><p>在<a href="http://lonepatient.top/2018/04/09/Deep_Learning_For_Computer_Vision_With_Python_PB_09.html">第9节</a>中，我们在将图像数据集转换为HDF5格式的过程中，计算了训练数据集中图像的红、绿、蓝三个颜色通道的平均值，之后，我们对每一张输入图像都减去对应通道平均值，这个过程我们称为数据归一化。即给定输入图像I及其对应R、G、B通道值，则可以通过:</p><ul><li class=" has-jax">R = R - <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>&amp;#x03BC;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>R</mi></mrow></msub></math>" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1" style="width: 1.468em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.224em; height: 0px; font-size: 117%;"><span style="position: absolute; clip: rect(1.529em, 1001.22em, 2.567em, -999.997em); top: -2.134em; left: 0em;"><span class="mrow" id="MathJax-Span-2"><span class="msubsup" id="MathJax-Span-3"><span style="display: inline-block; position: relative; width: 1.224em; height: 0px;"><span style="position: absolute; clip: rect(3.422em, 1000.61em, 4.46em, -999.997em); top: -4.026em; left: 0em;"><span class="mi" id="MathJax-Span-4" style="font-family: MathJax_Math-italic;">μ</span><span style="display: inline-block; width: 0px; height: 4.032em;"></span></span><span style="position: absolute; top: -3.904em; left: 0.614em;"><span class="texatom" id="MathJax-Span-5"><span class="mrow" id="MathJax-Span-6"><span class="mi" id="MathJax-Span-7" style="font-size: 70.7%; font-family: MathJax_Math-italic;">R</span></span></span><span style="display: inline-block; width: 0px; height: 4.032em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.14em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.354em; border-left: 0px solid; width: 0px; height: 0.932em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>μ</mi><mrow class="MJX-TeXAtom-ORD"><mi>R</mi></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-1">\mu_{R}</script></li><li class=" has-jax">G = G - <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-2-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>&amp;#x03BC;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>G</mi></mrow></msub></math>" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-8" style="width: 1.468em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.224em; height: 0px; font-size: 117%;"><span style="position: absolute; clip: rect(1.529em, 1001.22em, 2.567em, -999.997em); top: -2.134em; left: 0em;"><span class="mrow" id="MathJax-Span-9"><span class="msubsup" id="MathJax-Span-10"><span style="display: inline-block; position: relative; width: 1.224em; height: 0px;"><span style="position: absolute; clip: rect(3.422em, 1000.61em, 4.46em, -999.997em); top: -4.026em; left: 0em;"><span class="mi" id="MathJax-Span-11" style="font-family: MathJax_Math-italic;">μ</span><span style="display: inline-block; width: 0px; height: 4.032em;"></span></span><span style="position: absolute; top: -3.843em; left: 0.614em;"><span class="texatom" id="MathJax-Span-12"><span class="mrow" id="MathJax-Span-13"><span class="mi" id="MathJax-Span-14" style="font-size: 70.7%; font-family: MathJax_Math-italic;">G</span></span></span><span style="display: inline-block; width: 0px; height: 4.032em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.14em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.354em; border-left: 0px solid; width: 0px; height: 0.932em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>μ</mi><mrow class="MJX-TeXAtom-ORD"><mi>G</mi></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-2">\mu_{G}</script></li><li class=" has-jax">B = B - <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-3-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>&amp;#x03BC;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>B</mi></mrow></msub></math>" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-15" style="width: 1.468em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.224em; height: 0px; font-size: 117%;"><span style="position: absolute; clip: rect(1.529em, 1001.22em, 2.567em, -999.997em); top: -2.134em; left: 0em;"><span class="mrow" id="MathJax-Span-16"><span class="msubsup" id="MathJax-Span-17"><span style="display: inline-block; position: relative; width: 1.224em; height: 0px;"><span style="position: absolute; clip: rect(3.422em, 1000.61em, 4.46em, -999.997em); top: -4.026em; left: 0em;"><span class="mi" id="MathJax-Span-18" style="font-family: MathJax_Math-italic;">μ</span><span style="display: inline-block; width: 0px; height: 4.032em;"></span></span><span style="position: absolute; top: -3.904em; left: 0.614em;"><span class="texatom" id="MathJax-Span-19"><span class="mrow" id="MathJax-Span-20"><span class="mi" id="MathJax-Span-21" style="font-size: 70.7%; font-family: MathJax_Math-italic;">B</span></span></span><span style="display: inline-block; width: 0px; height: 4.032em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.14em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.354em; border-left: 0px solid; width: 0px; height: 0.932em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>μ</mi><mrow class="MJX-TeXAtom-ORD"><mi>B</mi></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-3">\mu_{B}</script></li></ul><p class=" has-jax">其中，<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-4-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>&amp;#x03BC;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>R</mi></mrow></msub><mo>,</mo><msub><mi>&amp;#x03BC;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>G</mi></mrow></msub><mo>,</mo><msub><mi>&amp;#x03BC;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>B</mi></mrow></msub></math>" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-22" style="width: 5.375em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.582em; height: 0px; font-size: 117%;"><span style="position: absolute; clip: rect(1.59em, 1004.58em, 2.628em, -999.997em); top: -2.195em; left: 0em;"><span class="mrow" id="MathJax-Span-23"><span class="msubsup" id="MathJax-Span-24"><span style="display: inline-block; position: relative; width: 1.224em; height: 0px;"><span style="position: absolute; clip: rect(3.422em, 1000.61em, 4.46em, -999.997em); top: -4.026em; left: 0em;"><span class="mi" id="MathJax-Span-25" style="font-family: MathJax_Math-italic;">μ</span><span style="display: inline-block; width: 0px; height: 4.032em;"></span></span><span style="position: absolute; top: -3.904em; left: 0.614em;"><span class="texatom" id="MathJax-Span-26"><span class="mrow" id="MathJax-Span-27"><span class="mi" id="MathJax-Span-28" style="font-size: 70.7%; font-family: MathJax_Math-italic;">R</span></span></span><span style="display: inline-block; width: 0px; height: 4.032em;"></span></span></span></span><span class="mo" id="MathJax-Span-29" style="font-family: MathJax_Main;">,</span><span class="msubsup" id="MathJax-Span-30" style="padding-left: 0.186em;"><span style="display: inline-block; position: relative; width: 1.224em; height: 0px;"><span style="position: absolute; clip: rect(3.422em, 1000.61em, 4.46em, -999.997em); top: -4.026em; left: 0em;"><span class="mi" id="MathJax-Span-31" style="font-family: MathJax_Math-italic;">μ</span><span style="display: inline-block; width: 0px; height: 4.032em;"></span></span><span style="position: absolute; top: -3.843em; left: 0.614em;"><span class="texatom" id="MathJax-Span-32"><span class="mrow" id="MathJax-Span-33"><span class="mi" id="MathJax-Span-34" style="font-size: 70.7%; font-family: MathJax_Math-italic;">G</span></span></span><span style="display: inline-block; width: 0px; height: 4.032em;"></span></span></span></span><span class="mo" id="MathJax-Span-35" style="font-family: MathJax_Main;">,</span><span class="msubsup" id="MathJax-Span-36" style="padding-left: 0.186em;"><span style="display: inline-block; position: relative; width: 1.224em; height: 0px;"><span style="position: absolute; clip: rect(3.422em, 1000.61em, 4.46em, -999.997em); top: -4.026em; left: 0em;"><span class="mi" id="MathJax-Span-37" style="font-family: MathJax_Math-italic;">μ</span><span style="display: inline-block; width: 0px; height: 4.032em;"></span></span><span style="position: absolute; top: -3.904em; left: 0.614em;"><span class="texatom" id="MathJax-Span-38"><span class="mrow" id="MathJax-Span-39"><span class="mi" id="MathJax-Span-40" style="font-size: 70.7%; font-family: MathJax_Math-italic;">B</span></span></span><span style="display: inline-block; width: 0px; height: 4.032em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.201em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.354em; border-left: 0px solid; width: 0px; height: 0.932em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>μ</mi><mrow class="MJX-TeXAtom-ORD"><mi>R</mi></mrow></msub><mo>,</mo><msub><mi>μ</mi><mrow class="MJX-TeXAtom-ORD"><mi>G</mi></mrow></msub><mo>,</mo><msub><mi>μ</mi><mrow class="MJX-TeXAtom-ORD"><mi>B</mi></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-4">\mu_{R},\mu_{G},\mu_{B}</script>是三个颜色通道平均值。图10.1显示了从输入图像中减去RGB平均值的可视化效果。</p><p><a href="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/18-7-25/9655360.jpg" class="fancybox fancybox.image" rel="group"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/18-7-25/9655360.jpg" alt=""></a></p><center>图10.1 左：原始图像，右：均值处理之后的图像</center><p>在代码实现零均值过程之前，我们首先看看整个项目结构，即:</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line">--- pyimagesearch</span><br><span class="line">| |--- __init__.py</span><br><span class="line">| |--- callbacks</span><br><span class="line">| |--- nn</span><br><span class="line">| |--- preprocessing</span><br><span class="line">| | |--- __init__.py</span><br><span class="line">| | |--- aspectawarepreprocessor.py</span><br><span class="line">| | |--- imagetoarraypreprocessor.py</span><br><span class="line">| | |--- meanpreprocessor.py</span><br><span class="line">| | |--- simplepreprocessor.py</span><br><span class="line">| |--- utils</span><br></pre></td></tr></tbody></table></figure></div><p>在pyimagesearch项目中的preprocessing子模块中新建一个meanpreprocessor.py，并写入以下代码:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MeanPreprocessor</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,rMean,gMean,bMean)</span>:</span></span><br><span class="line">        <span class="comment"># 三个颜色通道的平均值</span></span><br><span class="line">        self.rMean = rMean</span><br><span class="line">        self.gMean = gMean</span><br><span class="line">        self.bMean = bMean</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>其中，三个参数分别是对应的三个颜色通道的均值。</p><p>接下来，让我们定义预处理函数:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(self,image)</span>:</span></span><br><span class="line">    <span class="comment"># cv2分割得到的是BGR，而不是RGB</span></span><br><span class="line">    (B,G,R) = cv2.split(image.astype(<span class="string">"float32"</span>))</span><br><span class="line">    <span class="comment"># 减去对应通道的均值</span></span><br><span class="line">    R -= self.rMean</span><br><span class="line">    G -= self.gMean</span><br><span class="line">    B -= self.bMean</span><br><span class="line">    <span class="keyword">return</span> cv2.merge([B,G,R])</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>需要注意的是cv2.split函数是把图像划分成对应的B,G,R，而不是R,G,B。</p><h3 id="patch-Preprocessing"><a href="#patch-Preprocessing" class="headerlink" title="patch Preprocessing"></a>patch Preprocessing</h3><p>PatchPreprocessor类主要是在训练过程中随机抽取原始图像的MxN大小的特征图像。当输入的图像的维度比CNN模型要求的维度大时，我们就需要使用到patch Preprocessing——这是降低过拟合的常用技术，因此也是一种正则化形式。我们不会在训练过程中使用整个图像，而是随机裁剪其中的一部分，并将其传递给网络(关于裁剪预处理的示例，请参见图10.2)。</p><p><a href="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/18-7-25/20639656.jpg" class="fancybox fancybox.image" rel="group"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/18-7-25/20639656.jpg" alt=""></a></p><center>图10.2 左：原始256x256图像， 右：随机裁剪之后的227x227图像</center><p>使用随机裁剪方法意味着网络永远不会看到完全相同的图像(除非随机发生)，类似于数据增强。在<a href="http://lonepatient.top/2018/04/09/Deep_Learning_For_Computer_Vision_With_Python_PB_09.html">第9节</a>中，我们将kaggle上猫狗比赛数据集保存为HDF5数据集，其中原始数据的大小为256x256。然而，我们将在本章后面实现的AlexNet架构只能接受大小为227x227的图像。</p><p>那么，该如何处理？直接应用simplepreprocessor将每个256x256图像调整到227x227?从图10.2中，我们看到这样会损失图像信息，合理的做法是在训练过程中将256x256图像中随机裁剪出一个227x227特征图像——事实上，一方面类似于数据增强，另一方面这个过程正是Krizhevsky等人在ImageNet数据集中训练AlexNet的方式。</p><p>与其他图像预处理程序一样，patchpreprocessor预处理代码放在pyimagesearch的预处理子模块中，即：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line">--- pyimagesearch</span><br><span class="line">| |--- __init__.py</span><br><span class="line">| |--- callbacks</span><br><span class="line">| |--- nn</span><br><span class="line">| |--- preprocessing</span><br><span class="line">| | |--- __init__.py</span><br><span class="line">| | |--- aspectawarepreprocessor.py</span><br><span class="line">| | |--- imagetoarraypreprocessor.py</span><br><span class="line">| | |--- meanpreprocessor.py</span><br><span class="line">| | |--- patchpreprocessor.py</span><br><span class="line">| | |--- simplepreprocessor.py</span><br><span class="line">| |--- utils</span><br></pre></td></tr></tbody></table></figure></div><p>打开patchpreprocessor.py，并写入以下代码：<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.image <span class="keyword">import</span> extract_patches_2d</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PatchPreprocessor</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,width,height)</span>:</span></span><br><span class="line">        <span class="comment"># 目标图像的宽和高</span></span><br><span class="line">        self.width = width</span><br><span class="line">        self.height = height</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>其中，width和heigh分别表示裁剪后的图像宽度和高度。</p><p>接下来，定义预处理函数:</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(self,image)</span>:</span></span><br><span class="line">    <span class="comment"># 随机裁剪出目标大小图像</span></span><br><span class="line">    <span class="keyword">return</span> extract_patches_2d(image,(self.height,self.width),</span><br><span class="line">                              max_patches = <span class="number">1</span>)[<span class="number">0</span>]</span><br></pre></td></tr></tbody></table></figure></div><p>给定需要返回的图像的宽度和高度，我们使用scikit-learn库的extract_patches_2d函数从原始图像中随机裁剪出指定大小的图像，其中参数max_patch =1，表明我们只需要输入图像中的一个随机patch。</p><p>PatchPreprocessor类看起来代码并不是很多，但它实际上是一种非常有效的方法，类似于数据增强，一定程度上可以降低过拟合。一般在训练过程中使用PatchPreprocessor。</p><h3 id="随机裁剪"><a href="#随机裁剪" class="headerlink" title="随机裁剪"></a>随机裁剪</h3><p>接下来，我们定义一个CropPreprocessor。在CNN的评估阶段，我们对输入图像的四个角+中心区域进行裁剪，然后进行相应的水平翻转，最后会产生10个样本(图10.3)。</p><p><a href="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/18-7-25/32050231.jpg" class="fancybox fancybox.image" rel="group"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/18-7-25/32050231.jpg" alt=""></a></p><center>图10.3 左： 原始256x256图像，右：裁剪得到的10张227x227图像</center><p>CNN模型将对这10个测试样本进行预测，产生10个结果，最后通过投票或者计算平均得到最终结果。利用这种过采样的方法，往往会增加1- 2%的分类准确率(在某些情况下，甚至更高)。</p><p>CropPreprocessor类也将存在于pyimagesearch的preprocessing子模块中:</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line">--- pyimagesearch</span><br><span class="line">| |--- __init__.py</span><br><span class="line">| |--- callbacks</span><br><span class="line">| |--- nn</span><br><span class="line">| |--- preprocessing</span><br><span class="line">| | |--- __init__.py</span><br><span class="line">| | |--- aspectawarepreprocessor.py</span><br><span class="line">| | |--- croppreprocessor.py</span><br><span class="line">| | |--- imagetoarraypreprocessor.py</span><br><span class="line">| | |--- meanpreprocessor.py</span><br><span class="line">| | |--- patchpreprocessor.py</span><br><span class="line">| | |--- simplepreprocessor.py</span><br><span class="line">| |--- utils</span><br></pre></td></tr></tbody></table></figure></div><p>打开croppreprocessor.py文件，写入以下代码:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GropPreprocessor</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,width,height,horiz = True,inter=cv2.INTER_AREA)</span>:</span></span><br><span class="line">        <span class="comment"># 保存目标参数</span></span><br><span class="line">        self.width  = width</span><br><span class="line">        self.heiggt = height</span><br><span class="line">        self.horiz  = horiz</span><br><span class="line">        self.inter  = inter</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>其中，参数分别为:</p><ul><li>width: 输出的图像宽度</li><li>heigh：输出的图像高度</li><li>horiz：是否进行水平翻转，默认为True</li><li>inter：openCV中用于调整大小的插值算法</li></ul><p>定义预处理方法:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(self,image)</span>:</span></span><br><span class="line">    crops = []</span><br><span class="line">    <span class="comment"># 原始图像的高跟宽</span></span><br><span class="line">    (h,w) = image.shape[:<span class="number">2</span>]</span><br><span class="line">    <span class="comment">#四个角</span></span><br><span class="line">    coords = [</span><br><span class="line">            [<span class="number">0</span>,<span class="number">0</span>,self.width,self.height],</span><br><span class="line">            [w - self.width,<span class="number">0</span>,w,self.height],</span><br><span class="line">            [w - self.width,h - self.height,w,h],</span><br><span class="line">            [<span class="number">0</span>,h - self.height,self.width,h]</span><br><span class="line">            ]</span><br><span class="line">    <span class="comment"># 计算中心区域</span></span><br><span class="line">    dW = int(<span class="number">0.5</span> * (w - self.width))</span><br><span class="line">    dH = int(<span class="number">0.5</span> * (h - self.height))</span><br><span class="line">    coords.append([dW,dH,w - dW,h - dH])</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>其中预处理主函数只有一个参数，就是输入的图像，然后我们计算四个角(左上角、右上角、右下角、左下角)的坐标(x，y)以及中心坐标。并提取对应的部分图像：<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="keyword">for</span> (startX,startY,endX,endY) <span class="keyword">in</span> coords:</span><br><span class="line">    <span class="comment"># 裁剪</span></span><br><span class="line">    crop = image[startY:endY,startX:endX]</span><br><span class="line">    <span class="comment"># 由于裁剪过程，可能会造成大小相差1左右，所以进行插值</span></span><br><span class="line">    crop = cv2.resize(crop,(self.width,self.height),</span><br><span class="line">                      interpolation = self.inter)</span><br><span class="line">    crops.append(crop)</span><br><span class="line"><span class="keyword">if</span> self.horiz:</span><br><span class="line">    <span class="comment"># 水平翻转</span></span><br><span class="line">    mirrors = [cv2.flip(x,<span class="number">1</span>) <span class="keyword">for</span> x <span class="keyword">in</span> crops]</span><br><span class="line">    crops.extend(mirrors)</span><br><span class="line"><span class="keyword">return</span> np.array(crops)</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>由于提取的图像大小会与目标大小存在1左右的差别，因此，我们进行调整大小。通过水平翻转，我们可以得到10张特征图像。</p><h2 id="HDF5数据集生成器"><a href="#HDF5数据集生成器" class="headerlink" title="HDF5数据集生成器"></a>HDF5数据集生成器</h2><p>在训练模型之前，我们首先需要定义一个类，用于从HDF5数据集中生成batch大小的图像数据和对应的标签。在<a href="http://lonepatient.top/2018/04/09/Deep_Learning_For_Computer_Vision_With_Python_PB_09.html">第9节</a>中讨论了如何将保存在磁盘上的一组图像转换为HDF5数据集——但是我们如何将它们重新返回?</p><p>在pyimagesearch的io子模块中定义一个HDF5DatasetGenerator类:</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line">--- pyimagesearch</span><br><span class="line">| |--- __init__.py</span><br><span class="line">| |--- callbacks</span><br><span class="line">| |--- io</span><br><span class="line">| | |--- __init__.py</span><br><span class="line">| | |--- hdf5datasetgenerator.py</span><br><span class="line">| | |--- hdf5datasetwriter.py</span><br><span class="line">| |--- nn</span><br><span class="line">| |--- preprocessing</span><br><span class="line">| |--- utils</span><br></pre></td></tr></tbody></table></figure></div><p>之前训练模型时，所有图像数据集都可以加载到内存中，这样我们就可以依赖Keras生成器工具来生成batch大小的图像和相应的标签。但是，由于我们的数据集太大，无法全部加载到内存，所以我们需要自己实现一个针对HDF5数据集的生成器。</p><p>打开hdf5datasetgenerator.py文件，写入以下代码:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HDF5DatasetGenerator</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__inti__</span><span class="params">(self,dbPath,batchSize,preprocessors = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 aug = None,binarize=True,classes=<span class="number">2</span>)</span>:</span></span><br><span class="line">        <span class="comment"># 保存参数列表</span></span><br><span class="line">        self.batchSize = batchSize</span><br><span class="line">        self.preprocessors = preprocessors</span><br><span class="line">        self.aug = aug</span><br><span class="line">        self.binarize = binarize</span><br><span class="line">        self.classes = classes</span><br><span class="line">        <span class="comment"># hdf5数据集</span></span><br><span class="line">        self.db = h5py.File(dbPath)</span><br><span class="line">        self.numImages = self.db[<span class="string">'labels'</span>].shape[<span class="number">0</span>]</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>其中，参数分别为:</p><ul><li>dbPath: HDF5数据集的路径</li><li>batchSize: batch数据量的大小</li><li>preprocessors:数据预处理列表，比如（MeanPreprocessor,ImageToArrayPreprocessor等等)。</li><li>aug: 默认为None，可以使用Keras中的ImageDataGenerator模块来进行数据增强。</li><li>binarize: 通常我们将类标签作为一个整数存储在HDF5数据集中，但是，如果我们使用categorical cross-entropy或者binary cross-entropy作为损失函数，那么我们需要把标签进行one-hot编码向量化——该参数主要说明是否进行二值化处理(默认为True)。</li><li>classes:类别个数，该数值决定了one-hot向量的shape</li></ul><p>接下来，我们需要定义一个生成器函数，它负责在训练网络时将batch大小的图像和类标签传递给keras.fit_generator函数。<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">(self,passes=np.inf)</span>:</span></span><br><span class="line">    epochs = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 默认是无限循环遍历</span></span><br><span class="line">    <span class="keyword">while</span> epochs &lt; passes:</span><br><span class="line">        <span class="comment"># 遍历数据</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> np.arange(<span class="number">0</span>,self.numImages,self.batchSize):</span><br><span class="line">            <span class="comment"># 从hdf5中提取数据集</span></span><br><span class="line">            images = self.db[<span class="string">'images'</span>][i: i+self.batchSize]</span><br><span class="line">            labels = self.db[<span class="string">'labels'</span>][i: i+self.batchSize]</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>其中,passes为epochs的总数，一般而言，我们不用关心passes数值大小，因为在训练过程中，往往我们会指定训练的epoch个数，或者使用early stopping等等。所以默认为np.inf，即该循环会一直进行，直到:</p><ul><li>1.模型训练达到终止条件。</li><li>2.手动停止(例如，ctrl + c)。</li></ul><p>从hdf5数据集中提取数据之后，我们需要对数据进行预处理和标签one-hot编码:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="keyword">if</span> self.binarize:</span><br><span class="line">    labels = np_utils.to_categorical(labels,self.calsses)</span><br><span class="line"><span class="comment"># 预处理</span></span><br><span class="line"><span class="keyword">if</span> self.preprocessors <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">    proImages = []</span><br><span class="line">    <span class="keyword">for</span> image <span class="keyword">in</span> images:</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> self.preprocessors:</span><br><span class="line">            image = p.preprocess(image)</span><br><span class="line">        proImages.append(image)</span><br><span class="line">    images = np.array(proImages)</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>同样也可以设置是否进行数据增强处理：<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="keyword">if</span> self.aug <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">    (images,labels) = next(self.aug.flow(images,</span><br><span class="line">        labels,batch_size = self.batchSize))</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>最后，我们将返回由图像和标签组成的二元祖数据,</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line">            <span class="comment"># 返回</span></span><br><span class="line">            <span class="keyword">yield</span> (images,labels)</span><br><span class="line">        epochs += <span class="number">1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">close</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="comment"># 关闭db</span></span><br><span class="line">    self.db.close()</span><br></pre></td></tr></tbody></table></figure></div><p>整个HDF5数据集的生成器已经完成，在实际应用中，我们往往需要额外的工具来帮助我们快速地处理数据集，尤其是那些太大而无法加载到内存的数据集。后面，我们在开发深度学习项目或者实验时，HDF5DatasetGenerator将会加速我们处理数据的速度。</p><h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><p>这部分，我们将实现Krizhevsky等人提出的具有开创性的AlexNet架构。完整的AlexNet网络架构如下图所示：</p><p><a href="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/18-7-25/30390821.jpg" class="fancybox fancybox.image" rel="group"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/18-7-25/30390821.jpg" alt=""></a></p><center>图10.4 AlexNet结构</center><p>为什么我们将输入图像的大小调整为227x227x3——这实际上是AlexNet架构要求的正确输入大小。实际上，Krizhevsky等人发表的原论文中使用的图像维度是224x224x3，但是，使用大小为11x1的卷积核遍历图像，会发现边界填充的结果是小数，这个显然是不对的，即(224-11) /4+1的结果是小数，通过推导实际上应该是227x227。</p><p><strong>说明</strong>：输出数据体在空间上的尺寸可以通过输入数据尺寸（W），卷积层中神经元的感受野尺寸（F），步长（S）和零填充的数量（P）的函数来计算，即数据体的空间尺寸为（W-F+2P)/s+1.</p><p>接下来，在pyimagesearch中nn的conv子模块中创建一个名为alexnet.py文件，如下目录结构:</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line">--- pyimagesearch</span><br><span class="line">| |--- __init__.py</span><br><span class="line">| |--- callbacks</span><br><span class="line">| |--- io</span><br><span class="line">| |--- nn</span><br><span class="line">| | |--- __init__.py</span><br><span class="line">| | |--- conv</span><br><span class="line">| | | |--- __init__.py</span><br><span class="line">| | | |--- alexnet.py</span><br><span class="line">...</span><br><span class="line">| |--- preprocessing</span><br><span class="line">| |--- utils</span><br></pre></td></tr></tbody></table></figure></div><p>打开alexnet.py，并写入以下代码:</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># 加载所需模块</span></span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> BatchNormalization</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Conv2D</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> MaxPooling2D</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Activation</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Flatten</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dropout</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="keyword">from</span> keras.regularizers <span class="keyword">import</span> l2</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br></pre></td></tr></tbody></table></figure></div><p>从加载的模块中可以看出，我们使用了BN和l2模块。AlexNet原文中使用的标准化是LRN（局部响应归一化），在实现中，我们使用更高级的BN算法，另外为了防止过拟合，我们将会卷积层和FC使用l2正则化。</p><p>下面，定义AlexNet结构：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AlexNet</span>:</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(width,height,depth,classes,reg=<span class="number">0.0002</span>)</span>:</span></span><br><span class="line">        <span class="comment"># 初始化序列模型</span></span><br><span class="line">        model = Sequential()</span><br><span class="line">        inputShape = (height,width,depth)</span><br><span class="line">        chanDim = <span class="number">-1</span></span><br><span class="line">        <span class="comment"># 主要识别keras的后端是thensorflow还是theano[目前默认都是thensorflow]</span></span><br><span class="line">        <span class="keyword">if</span> K.image_data_format() == <span class="string">"channels_first"</span>:</span><br><span class="line">            inputShape = (depth,height,width)</span><br><span class="line">            chanDim = <span class="number">1</span></span><br></pre></td></tr></tbody></table></figure></div><p>其中：</p><ul><li>width：输入图像的宽度</li><li>height：输入图像的高度</li><li>depth：输入图像的通道数，彩色为3，灰色为1</li><li>classes：类别总数</li><li>reg：正则化系数，对于较大、较深的网络，应用正则化有助于降低过拟合，同时提高准确度</li></ul><p>接下来，我们构建模型的主体部分。首先，定义网络中的第一个CONV =&gt; RELU =&gt;POOL 模块：<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># Block # 1 first CONV =&gt; RELU =&gt; POOL layer set</span></span><br><span class="line">model.add(Conv2D(<span class="number">96</span>,(<span class="number">11</span>,<span class="number">11</span>),strides=(<span class="number">4</span>,<span class="number">4</span>),input_shape=inputShape,</span><br><span class="line">                 padding=<span class="string">'same'</span>,kernel_regularizer=l2(reg)))</span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>))</span><br><span class="line">model.add(BatchNormalization(axis = chanDim))</span><br><span class="line">model.add(MaxPooling2D(pool_size = (<span class="number">3</span>,<span class="number">3</span>),strides=(<span class="number">2</span>,<span class="number">2</span>)))</span><br><span class="line">model.add(Dropout(<span class="number">0.25</span>))</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>第一个CONV层有96个过滤器，每个过滤器大小为11x11，平移步长为4，激活函数为RELU，并且对卷积层使用了l2正则化（l2正则化后面会一直应用在CONV层和FC层）。</p><p>从图10.4，可以看出卷积层之后，是标准化操作，原文论文中使用的标准化为LRN，这里我们使用更加高级的BN层。标准化之后，是MaxPooling2D层，pool层可以降低特征图像维度，并减少参数量。最后，我们还增加了dropout层，进一步降低过拟合。</p><p>下面，我们定义第二个block，主要是CONV =&gt; RELU =&gt; POOL，类似于与第一个block，一共有256个过滤器，每一个过滤器的大小为5x5，平移步长为1：<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># Block #2: second CONV =&gt; RELU =&gt; POOL layer set</span></span><br><span class="line">model.add(Conv2D(<span class="number">256</span>,(<span class="number">5</span>,<span class="number">5</span>),padding=<span class="string">'same'</span>,</span><br><span class="line">                 kernel_regularizer=l2(reg)))</span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>))</span><br><span class="line">model.add(BatchNormalization(axis=chanDim))</span><br><span class="line">model.add(MaxPooling2D(pool_size=(<span class="number">3</span>,<span class="number">3</span>),strides(<span class="number">2</span>,<span class="number">2</span>)))</span><br><span class="line">model.add(Dropout(<span class="number">0.25</span>))</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>定义第三个block，第三个block中叠加了多个CONV层，将学习到更深层次且丰富的特征：<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># Block #3: CONV =&gt; RELU =&gt; CONV =&gt; RELU =&gt; CONV =&gt; RELU</span></span><br><span class="line">model.add(Conv2D(<span class="number">384</span>,(<span class="number">3</span>,<span class="number">3</span>),padding=<span class="string">'same'</span>,</span><br><span class="line">                 kernel_regularizer=l2(reg)))</span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>))</span><br><span class="line">model.add(BatchNormalization(axis = chanDim))</span><br><span class="line">model.add(Conv2D(<span class="number">384</span>,(<span class="number">3</span>,<span class="number">3</span>),padding=<span class="string">'same'</span>,</span><br><span class="line">                 kernel_regularizer=l2(reg)))</span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>))</span><br><span class="line">model.add(BatchNormalization(axis=chanDim))</span><br><span class="line">model.add(Conv2D(<span class="number">256</span>,(<span class="number">3</span>,<span class="number">3</span>),padding=<span class="string">'same'</span>,</span><br><span class="line">                 kernel_regularizer=l2(reg)))</span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>))</span><br><span class="line">model.add(BatchNormalization(axis = chanDim))</span><br><span class="line">model.add(MaxPooling2D(pool_size=(<span class="number">3</span>,<span class="number">3</span>),strides=(<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">model.add(Dropout(<span class="number">0.25</span>))</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>前两个CONV层主要是由384个大小为3x3的过滤器组成，而第三个CONV由256个大小为3x3的过滤器组成，步长全部为1。</p><p>接着，我们拼接两个全连接层，每个全连接层都由4096神经元组成:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># Block #4: first set of FC =&gt; RELU layers</span></span><br><span class="line">model.add(Flatten()) <span class="comment"># 拉平</span></span><br><span class="line"><span class="comment"># 全连接层</span></span><br><span class="line">model.add(Dense(<span class="number">4096</span>,kernel_regularizer=l2(reg)))</span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>))</span><br><span class="line">model.add(BatchNormalization(axis = chanDim))</span><br><span class="line">model.add(Dropout(<span class="number">0.5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Block #5: second set of FC =&gt; RELU layers</span></span><br><span class="line">model.add(Dense(<span class="number">4096</span>,kernel_regularizer=l2(reg)))</span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>))</span><br><span class="line">model.add(BatchNormalization(axis = chanDim))</span><br><span class="line">model.add(Dropout(<span class="number">0.5</span>))</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>在全连接FC层之后，紧接这BN层和Dropout层（一个较大的概率值，比如0.5）。</p><p>最后，我们定义一个softmax分类器，并将模型结果返回给调用函数:</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># softmax 分类器</span></span><br><span class="line">model.add(Dense(classes,kernel_regularizer=l2(reg)))</span><br><span class="line">model.add(Activation(<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#返回模型结构</span></span><br><span class="line"><span class="keyword">return</span>  model</span><br></pre></td></tr></tbody></table></figure></div><p>从整个构建AlexNet网络架构过程中可以看到，我们完成是按照图10.1所示的体系结构进行编写代码，你会发现实现AlexNet模型是一个相当简单的过程。</p><h2 id="训练猫狗数据集"><a href="#训练猫狗数据集" class="headerlink" title="训练猫狗数据集"></a>训练猫狗数据集</h2><p>前面我们已经定义好AlexNet模型架构，接下来，我们开始训练猫狗数据集，新建一个名为train_alexnet.py，写入以下代码:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment">#加载所需模块</span></span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line">matplotlib.use(<span class="string">'Agg'</span>)</span><br><span class="line"><span class="keyword">from</span> config <span class="keyword">import</span> dogs_vs_cats_config <span class="keyword">as</span> config</span><br><span class="line"><span class="keyword">from</span> pyimagesearch.preprocessing <span class="keyword">import</span> imagetoarraypreprocessor <span class="keyword">as</span> IAP</span><br><span class="line"><span class="keyword">from</span> pyimagesearch.preprocessing <span class="keyword">import</span> simplespreprocessor <span class="keyword">as</span> SP</span><br><span class="line"><span class="keyword">from</span> pyimagesearch.preprocessing <span class="keyword">import</span> patchpreprocessor <span class="keyword">as</span> PP</span><br><span class="line"><span class="keyword">from</span> pyimagesearch.preprocessing <span class="keyword">import</span> meanpreprocessor <span class="keyword">as</span> MP</span><br><span class="line"><span class="keyword">from</span> pyimagesearch.callbacks <span class="keyword">import</span> trainingmonitor <span class="keyword">as</span> TM</span><br><span class="line"><span class="keyword">from</span> pyimagesearch.io <span class="keyword">import</span> hdf5datasetgenerator <span class="keyword">as</span> HDF</span><br><span class="line"><span class="keyword">from</span> pyimagesearch.nn.conv <span class="keyword">import</span> alexnet</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> Adam</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> os</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>在大多数计算机视觉任务中，我们都建议使用数据增强技术，因此，我们对猫狗数据集进行数据增强处理:</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 数据增强</span></span><br><span class="line">aug = ImageDataGenerator(rotation_range = <span class="number">20</span>,zoom_range = <span class="number">0.15</span>,</span><br><span class="line">                         width_shift_range = <span class="number">0.2</span>,height_shift_range = <span class="number">0.2</span>,</span><br><span class="line">                         shear_range=<span class="number">0.15</span>,horizontal_flip=<span class="keyword">True</span>,</span><br><span class="line">                         fill_mode=<span class="string">'nearest'</span>)</span><br></pre></td></tr></tbody></table></figure></div><p>初始化数据预处理模块：<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment">#加载ＲＧＢ均值文件</span></span><br><span class="line">means = json.loads(open(config.DATASET_MEAN).read())</span><br><span class="line"><span class="comment"># 预处理</span></span><br><span class="line">sp = SP.SimplePreprocessor(<span class="number">227</span>,<span class="number">227</span>)</span><br><span class="line">pp = PP.PatchPreprocessor(<span class="number">227</span>,<span class="number">227</span>)</span><br><span class="line">mp = MP.MeanPreprocessor(means[<span class="string">'R'</span>],means[<span class="string">'G'</span>],means[<span class="string">'B'</span>])</span><br><span class="line">iap = IAP.ImageToArrayPreprocessor()</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>这里，我们初始化了三个预处理方法，其中：</p><ul><li>means：RGB颜色通道平均值数据</li><li>sp：将图像大小调整到227x227，该函数主要对验证数据集</li><li>pp：随机从原始256x256图像中裁剪出227x227大小特征图像，该函数主要是针对训练数据集</li><li>mp： 零均值化，将特征图像减去对应通道的平均值</li><li>iap： 将特征图像转换为keras支持的数组形式</li></ul><p>初始化训练数据集和验证数据集生成器：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment">#初始化训练数据集和验证数据集生成器</span></span><br><span class="line">trainGen = HDF.HDF5DatasetGenerator(dbPath=config.TRAIN_HDF5,batchSize=<span class="number">128</span>,aug=aug,preprocessors= [pp,mp,iap],classes = <span class="number">2</span>)</span><br><span class="line">valGen = HDF.HDF5DatasetGenerator(config.VAL_HDF5,<span class="number">128</span>,preprocessors=[sp,mp,iap],classes =<span class="number">2</span>)</span><br></pre></td></tr></tbody></table></figure></div><p>在这里<strong>需要注意</strong>训练数据集生成器和验证数据集生成器的参数值区别。</p><p>初始化Adam优化器和AlexNet模型:</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 初始化优化器</span></span><br><span class="line">print(<span class="string">"[INFO] compiling model..."</span>)</span><br><span class="line">opt = Adam(lr=<span class="number">1e-3</span>)</span><br><span class="line"><span class="comment">#初始化模型</span></span><br><span class="line">model = alexnet.AlexNet.build(width=<span class="number">227</span>,height=<span class="number">227</span>,depth=<span class="number">3</span>,</span><br><span class="line">                      classes=<span class="number">2</span>,reg=<span class="number">0.0002</span>)</span><br><span class="line">model.compile(loss = <span class="string">'binary_crossentropy'</span>,optimizer=opt,</span><br><span class="line">              metrics = [<span class="string">'accuracy'</span>])</span><br><span class="line"><span class="comment"># callbacks</span></span><br><span class="line">path = os.path.sep.join([config.OUTPUT_PATH,<span class="string">"{}.png"</span>.format(os.getpid())])</span><br><span class="line">callbacks = [TM.TrainingMonitor(path)]<span class="comment"># 回调函数</span></span><br></pre></td></tr></tbody></table></figure></div><p>在<a href="http://lonepatient.top/2018/03/25/Deep_Learning_For_Computer_Vision_With_Python_PB_07.html">第7节</a>中我们学习了几种自适应学习率优化算法，本节中，我们将使用Adam优化算法（默认学习率为0.001）。选择Adam（而不是SGD)，主要由于:</p><ul><li>1.尝试更高级的优化算法。</li><li>2.针对该分类任务，Adam比SGD表现得更好。</li></ul><p>前面，我们分析了AlexNet模型输入的图像的shape应为(227,227,3)。另外，对于一般二元分类问题常用的loss函数为binary cross-entropy，多分类问题常用的loss函数为categorical cross-entropy，因此，这里我们使用的loss函数为binary cross-entropy。callbacks回调参数主要是为了方便在网络进行训练时监视其性能。</p><p>训练网络:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 训练网络</span></span><br><span class="line">model.fit_generator(</span><br><span class="line">        trainGen.generator(),</span><br><span class="line">        steps_per_epoch = trainGen.numImages // <span class="number">128</span>,</span><br><span class="line">        validation_data = valGen.generator(),</span><br><span class="line">        validation_steps = valGen.numImages // <span class="number">128</span>,</span><br><span class="line">        epochs = <span class="number">75</span>,</span><br><span class="line">        max_queue_size = <span class="number">128</span> * <span class="number">2</span>,</span><br><span class="line">        callbacks = callbacks,</span><br><span class="line">        verbose = <span class="number">1</span>)</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>我们的数据是从生成器返回的，因此，我们这里使用的是.fit_generator方法</p><p>训练完模型之后，我们将得到的模型保存在磁盘中:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 保存模型文件</span></span><br><span class="line">print(<span class="string">"[INFO] serializing model ...."</span>)</span><br><span class="line">model.save(config.MODEL_PATH,overwrite = <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭HDF5数据</span></span><br><span class="line">trainGen.close()</span><br><span class="line">valGen.close()</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>上面，我们完成从数据获取、数据预处理和训练模型，接下来，执行以下命令:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="meta">$</span><span class="bash"> python train_alexnet.py</span></span><br><span class="line">Epoch 73/75</span><br><span class="line">415s - loss: 0.4862 - acc: 0.9126 - val_loss: 0.6826 - val_acc: 0.8602</span><br><span class="line">Epoch 74/75</span><br><span class="line">408s - loss: 0.4865 - acc: 0.9166 - val_loss: 0.6894 - val_acc: 0.8721</span><br><span class="line">Epoch 75/75</span><br><span class="line">401s - loss: 0.4813 - acc: 0.9166 - val_loss: 0.4195 - val_acc: 0.9297</span><br><span class="line">[INFO] serializing model...</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>当75次训练迭代结束后，我们可以得到如图10.5的训练和验证的loss/accuracy曲线图。总的来说，我们可以看到训练次数和准确率之间具有一定的相关性，另外，从图中我们可以看到验证loss并没还稳定下来，那么我们可以加大epochs次数，当然我们也可以通过应用一些学习率衰减方法，使得只在75次迭代中，验证loss趋向于稳定。从结果中，我们可以看到验证集的精度达到了92.97%。</p><p><a href="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/18-7-25/67137779.jpg" class="fancybox fancybox.image" rel="group"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/18-7-25/67137779.jpg" alt=""></a></p><center>图10.5 AlexNet模型性能曲线</center><p>接下来，我们对测试集做两次实验，即：</p><ul><li>1.直接模型对原始测试集进行预测，并查看结果</li><li>2.先对测试集进行过采样方法，然后使用对采样得到数据进行预测，并查看结果</li></ul><p>你将从结果中看到，对测试集使用过采样方法，精度会提高了1-3%左右。</p><h2 id="性能评估"><a href="#性能评估" class="headerlink" title="性能评估"></a>性能评估</h2><p>新建一个名为crop_accuracy.py的文件，如以下结构:</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line">--- dogs_vs_cats</span><br><span class="line">| |--- config</span><br><span class="line">| |--- build_dogs_vs_cats.py</span><br><span class="line">| |--- crop_accuracy.py</span><br><span class="line">| |--- extract_features.py</span><br><span class="line">| |--- train_alexnet.py</span><br><span class="line">| |--- train_model.py</span><br><span class="line">| |--- output</span><br></pre></td></tr></tbody></table></figure></div><p>打开crop_accuracy.py，并写入以下代码:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> config <span class="keyword">import</span> dogs_vs_cats_config <span class="keyword">as</span> config</span><br><span class="line"><span class="keyword">from</span> pyimagesearch.preprocessing <span class="keyword">import</span> imagetoarraypreprocessor <span class="keyword">as</span> IAP</span><br><span class="line"><span class="keyword">from</span> pyimagesearch.preprocessing <span class="keyword">import</span> simplespreprocessor <span class="keyword">as</span> SP</span><br><span class="line"><span class="keyword">from</span> pyimagesearch.preprocessing <span class="keyword">import</span> patchpreprocessor <span class="keyword">as</span> PP</span><br><span class="line"><span class="keyword">from</span> pyimagesearch.preprocessing <span class="keyword">import</span> meanpreprocessor <span class="keyword">as</span> MP</span><br><span class="line"><span class="keyword">from</span> pyimagesearch.preprocessing <span class="keyword">import</span> croppreprocessor <span class="keyword">as</span> CP</span><br><span class="line"><span class="keyword">from</span> pyimagesearch.io <span class="keyword">import</span> hdf5datasetgenerator <span class="keyword">as</span> HDF</span><br><span class="line"><span class="keyword">from</span> pyimagesearch.utils.ranked <span class="keyword">import</span> rank5_accuracy</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> load_model</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> progressbar</span><br><span class="line"><span class="keyword">import</span> json</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>从磁盘中加载RGB均值数据，并初始化图像预处理和加载预先训练好的AlexNet网络:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 加载RGB均值数据</span></span><br><span class="line">means = json.loads(open(config.DATASET_MEAN).read())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化预处理</span></span><br><span class="line">sp = SP.SimplePreprocessor(<span class="number">227</span>,<span class="number">227</span>)</span><br><span class="line">mp = MP.MeanPreprocessor(means[<span class="string">'R'</span>],means[<span class="string">'G'</span>],means[<span class="string">'B'</span>])</span><br><span class="line">cp = CP.CropPreprocessor(<span class="number">227</span>,<span class="number">227</span>)</span><br><span class="line">iap = IAP.ImageToArrayPreprocessor()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载训练好的模型</span></span><br><span class="line">print(<span class="string">"[INFO] loading model ..."</span>)</span><br><span class="line">model = load_model(config.MODEL_PATH)</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>在应用过采样方法之前，我们先在测试集上获得一个baseline，即模型对原始的测试图像进行预测:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 初始化测试数据集生成器，并进行预测</span></span><br><span class="line">print(<span class="string">"[INFO] predicting on test data (no crops)..."</span>)</span><br><span class="line">testGen = HDF.HDF5DatasetGenerator(config.TEST_HDF5,<span class="number">64</span>,</span><br><span class="line">                               preprocessors = [sp,mp,iap],</span><br><span class="line">                               classes = <span class="number">2</span>)</span><br><span class="line">predictions = model.predict_generator(testGen.generator(),</span><br><span class="line">                                      steps = testGen.numImages // <span class="number">64</span>,</span><br><span class="line">                                      max_queue_size = <span class="number">64</span> * <span class="number">2</span>)</span><br><span class="line"><span class="comment">#计算rank-1和rank-5准确度</span></span><br><span class="line">(rank1,_) = rank5_accuracy(predictions,testGen.db[<span class="string">'labels'</span>])</span><br><span class="line">print(<span class="string">"[INFO] rank-1: {:.2f}%"</span>.format(rank1 * <span class="number">100</span>))</span><br><span class="line">testGen.close()</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>猫狗数据集是一个二分类问题，因此我们只关心rank-1准确度，如果计算rank-5准确度，那么会得到100%的rank-5准确度，这是无意义的。</p><p>接下来，我们对测试集数据进行过采样:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 重新初始化生成器</span></span><br><span class="line"><span class="comment"># 'SimplePreprocessor'</span></span><br><span class="line">testGen = HDF.HDF5DatasetGenerator(config.TEST_HDF5,<span class="number">64</span>,</span><br><span class="line">                               preprocessors = [mp],classes = <span class="number">2</span>)</span><br><span class="line">predictions = []</span><br><span class="line"><span class="comment"># 初始化进度条</span></span><br><span class="line">widgets = [<span class="string">'Evaluating: '</span>,progressbar.Percentage(),<span class="string">" "</span>,</span><br><span class="line">           progressbar.Bar(),<span class="string">" "</span>,progressbar.ETA()]</span><br><span class="line">pbar = progressbar.ProgressBar(maxval = testGen.numImages // <span class="number">64</span>,</span><br><span class="line">                               widgets=widgets).start()</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>遍历每一张图像，并进行10-cropping方法，得到10张特征图像:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 遍历测试集</span></span><br><span class="line"><span class="keyword">for</span> (i,(images,labels)) <span class="keyword">in</span> enumerate(testGen.generator(passes=<span class="number">1</span>)):</span><br><span class="line">    <span class="comment">#遍历图像数据集</span></span><br><span class="line">    <span class="keyword">for</span> image <span class="keyword">in</span> images:</span><br><span class="line">        <span class="comment"># 10-cropping，返回10个特征图像数据</span></span><br><span class="line">        crops = cp.preprocess(image)</span><br><span class="line">        crops = np.array([iap.preprocess(c) <span class="keyword">for</span> c <span class="keyword">in</span> crops],</span><br><span class="line">                          dtype = <span class="string">'float32'</span>)</span><br><span class="line">        pred = model.predict(crops)</span><br><span class="line">        predictions.append(pred.mean(axis = <span class="number">0</span>))</span><br><span class="line">    <span class="comment"># 更新进度条</span></span><br><span class="line">    pbar.update(i)</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>训练时候，一般我们会设置HDF5DatasetGenerator为永久循环，直到满足一定条件后自动停止（通常在训练时设置最大的迭代次数）。但是，在测试过程中，我们只需循环一次，因此，passes =1。</p><p>对每个输入图像进行过采样处理——在原始256x256图像的基础上通过裁剪得到为10个227x227图像的数组，然后模型对每一个样本进行预测，最后计算平均值作为最终结果。</p><p>计算rank-1准确度:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 计算rank-1准确度</span></span><br><span class="line">pbar.finish()</span><br><span class="line">print(<span class="string">"[INFO] predicting on test data (with crops)...."</span>)</span><br><span class="line">(rank1,_) = rank5_accuracy(predictions,testGen.db[<span class="string">'labels'</span>])</span><br><span class="line">print(<span class="string">"[INFO'] rank-1: {:.2f}%"</span>.format(rank1 * <span class="number">100</span>))</span><br><span class="line">testGen.close()</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>执行以下命令，查看性能结果:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="meta">$</span><span class="bash"> python crop_accuracy.py</span></span><br><span class="line">[INFO] loading model...</span><br><span class="line">[INFO] predicting on test data (no crops)...</span><br><span class="line">[INFO] rank-1: 92.60%</span><br><span class="line">Evaluating: 100% |####################################| Time: 0:01:12</span><br><span class="line">[INFO] predicting on test data (with crops)...</span><br><span class="line">[INFO] rank-1: 94.00%</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>如结果所示，第1个实验中，模型在测试集上的准确率达到了92.60%。但是，在第2个实验中，通过10-cropping的过采样方法，我们能够将分类准确率提高到94.00%，增加了1.4%。简单的一个操作，让精度提高了1-3%左右。</p><h2 id="Top30"><a href="#Top30" class="headerlink" title="Top30"></a>Top30</h2><p>如果你观察了Kaggle猫狗比赛的排行榜，你会发现，要想进入前30名，我们差不多需要96.69%的准确率，而我们目前的方法是无法达到的。那么该如何提高呢？</p><p>我们主要使用转移学习——通过特征提取。虽然ImageNet数据集包含1000个对象类别，但其中很大一部分包含了狗的种类和猫的种类。说明在ImageNet上训练的网络不仅能告诉你一个图片是狗还是猫，而且还能告诉你这个动物是什么品种。鉴于在ImageNet上训练的网络具有区分这些细粒度特征的能力，我们可能会想到直接对预先训练好的网络中提取出来的特征向量训练一个常见的机器学习模型，很可能会获得更好的结果。</p><p>接下来，我们来实现这个过程，我们首先从训练好的ResNet模型中提取特征，然后对这些特征向量训练逻辑回归分类器。</p><h2 id="提取特征-ResNet"><a href="#提取特征-ResNet" class="headerlink" title="提取特征-ResNet"></a>提取特征-ResNet</h2><p>在<a href="http://lonepatient.top/2018/02/25/Deep_Learning_For_Computer_Vision_With_Python_PB_03.html">第3节</a>中，我们讨论了特征提取的详细内容，这里，就不做详细的描述。我们新建一个extract_features.py，并写入一下代码:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># 加载所需要模块</span></span><br><span class="line"><span class="keyword">from</span> keras.applications <span class="keyword">import</span> ResNet50</span><br><span class="line"><span class="keyword">from</span> keras.applications <span class="keyword">import</span> imagenet_utils</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.image <span class="keyword">import</span> img_to_array</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.image <span class="keyword">import</span> load_img</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"><span class="keyword">from</span> pyimagesearch.io <span class="keyword">import</span> hdf5datasetwriter <span class="keyword">as</span> HDF</span><br><span class="line"><span class="keyword">from</span> imutils <span class="keyword">import</span> paths</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> progressbar</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> os</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>使用keras加载预先训练好的ResNet50模型，若本地没有模型文件，则keras会自动进行下载。</p><p>解析命令行参数:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 命令行参数</span></span><br><span class="line">ap = argparse.ArgumentParser()</span><br><span class="line">ap.add_argument(<span class="string">'-d'</span>,<span class="string">"--dataset"</span>,required = <span class="keyword">True</span>,</span><br><span class="line">                help = <span class="string">'path to input dataset'</span>)</span><br><span class="line">ap.add_argument(<span class="string">"-0"</span>,<span class="string">'--output'</span>,required = <span class="keyword">True</span>,</span><br><span class="line">                help = <span class="string">'path ot output hdf5 file'</span>)</span><br><span class="line">ap.add_argument(<span class="string">'-b'</span>,<span class="string">'--batch_size'</span>,type = int,default = <span class="number">16</span>,</span><br><span class="line">                help=<span class="string">'batch size of images to ba passed through network'</span>)</span><br><span class="line">ap.add_argument(<span class="string">'-s'</span>,<span class="string">'--buffer_size'</span>,type=int,default=<span class="number">1000</span>,</span><br><span class="line">                help = <span class="string">'size of feature extraction buffer'</span>)</span><br><span class="line">args = vars(ap.parse_args())</span><br><span class="line"><span class="comment"># batch</span></span><br><span class="line">bs = args[<span class="string">'batch_size'</span>]</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>其中：</p><ul><li>datset：输入数据的路径</li><li>output： 输出路径</li><li>batch_size： batch数量的大小，默认为16</li><li>buffer_size: 默认1000，每1000次进行一次 操作</li></ul><p>从磁盘中读取数据集并提取相应的标签数据:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line">print(<span class="string">"[INFO] loading images..."</span>)</span><br><span class="line">imagePaths = list(paths.list_images(args[<span class="string">'dataset'</span>]))</span><br><span class="line"><span class="comment"># 混洗数据</span></span><br><span class="line">random.shuffle(imagePaths)</span><br><span class="line"><span class="comment"># 标签获取</span></span><br><span class="line">labels = [p.split(os.path.sep)[<span class="number">-1</span>].split(<span class="string">"."</span>)[<span class="number">0</span>] <span class="keyword">for</span> p <span class="keyword">in</span> imagePaths]</span><br><span class="line"><span class="comment"># 编码编码化</span></span><br><span class="line">le = LabelEncoder()</span><br><span class="line">labels = le.fit_transform(labels)</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>可以磁盘中加载预训练的ResNet50模型权重(不包括FC层):<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 加载ResNet50模型</span></span><br><span class="line">print(<span class="string">"[INFO] loading network..."</span>)</span><br><span class="line"><span class="comment"># imagenet上训练的权重</span></span><br><span class="line">model = ResNet50(weights = <span class="string">'imagenet'</span>,include_top=<span class="keyword">False</span>)</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>为了将从ResNet50提取的特征数据存储到磁盘，我们需要实例化aHDF5DatasetWriter对象:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 初始化HDF5数据集，保存数据标签</span></span><br><span class="line">ResNet50的最后一个average pooling层的维度是<span class="number">2048</span></span><br><span class="line">dataset = HDF.HDF5DatasetWriter((len(imagePaths),<span class="number">2048</span>),</span><br><span class="line">                            args[<span class="string">'output'</span>],dataKey=<span class="string">'feature'</span>,buffSize=args[<span class="string">'buffer_size'</span>])</span><br><span class="line">dataset.storeClassLabels(le.classes_)</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>初始化progressbar，以便跟踪特征提取过程:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 初始化进度条</span></span><br><span class="line">widgets = [<span class="string">'Extracting Features: '</span>,progressbar.Percentage(),<span class="string">' '</span>,</span><br><span class="line">           progressbar.Bar(),<span class="string">' '</span>,progressbar.ETA()]</span><br><span class="line">pbar = progressbar.ProgressBar(maxval = len(imagePaths),</span><br><span class="line">                               widgets = widgets).start()</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>使用CNN从数据集中提取特征与在第3章中一样。首先，我们对图像路径数据进行批量循环:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 遍历batch图像数据集</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> np.arange(<span class="number">0</span>,len(imagePaths),bs):</span><br><span class="line">    <span class="comment">#提取图像和标签</span></span><br><span class="line">    batchPaths = imagePaths[i:i+bs]</span><br><span class="line">    batchLabels = labels[i:i+bs]</span><br><span class="line">    batchImages = []</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>然后对每张图像进行预处理:</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 对每张图像进行处理</span></span><br><span class="line"><span class="keyword">for</span> (j,imagePath) <span class="keyword">in</span> enumerate(batchPaths):</span><br><span class="line">    image = load_img(imagePath,target_size = (<span class="number">224</span>,<span class="number">224</span>))</span><br><span class="line">    image = img_to_array(image)</span><br><span class="line">    <span class="comment"># 增加一维度</span></span><br><span class="line">    image = np.expand_dims(image,axis = <span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 提取imageNet数据集中RGB均值</span></span><br><span class="line">    image = imagenet_utils.preprocess_input(image)</span><br><span class="line">    batchImages.append(image)</span><br></pre></td></tr></tbody></table></figure></div><p>从ResNet50的最后一个池化层中提取特征数据:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line">batchImages = np.vstack(batchImages)</span><br><span class="line"><span class="comment"># 提取最后一个pool层特征图像</span></span><br><span class="line">features = model.predict(batchImages,batch_size=bs)</span><br><span class="line"><span class="comment"># 拉平</span></span><br><span class="line">features = features.reshape((features.shape[<span class="number">0</span>],<span class="number">2048</span>))</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>然后将这些提取出来的特征向量保存到HDF5数据集中:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line">    <span class="comment"># hdf5数据集中增加特征和标签</span></span><br><span class="line">    dataset.add(features,batchLabels)</span><br><span class="line">    pbar.update(i)</span><br><span class="line"><span class="comment"># 关闭数据</span></span><br><span class="line">dataset.close()</span><br><span class="line">pbar.finish()</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>执行以下命令，完成整个特征提取步骤:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="meta">$</span><span class="bash"> python extract_features.py --dataset ../datasets/kaggle_dogs_vs_cats/train \</span></span><br><span class="line">--output ../datasets/kaggle_dogs_vs_cats/hdf5/features.hdf5</span><br><span class="line">[INFO] loading images...</span><br><span class="line">[INFO] loading network...</span><br><span class="line">Extracting Features: 100% |####################################| Time: 0:06:18</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>执行完命令后，查看输出目录:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="meta">$</span><span class="bash"> ls -l ../datasets/kaggle_dogs_vs_cats/hdf5/features.hdf5</span></span><br><span class="line">-rw-rw-r-- adrian 409806272 Jun 3 07:17 output/dogs_vs_cats_features.hdf5</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>基于这些特征向量，我们将训练一个逻辑回归分类器。</p><h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><p>新建一个名为train_model.py文件，并写入以下代码:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 加载所需模块</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> h5py</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>解析命令行参数:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment">#命令行参数</span></span><br><span class="line">ap = argparse.ArgumentParser()</span><br><span class="line">ap.add_argument(<span class="string">"-d"</span>,<span class="string">"--db"</span>,required = <span class="keyword">True</span>,</span><br><span class="line">                help = <span class="string">'path HDF5 datasetbase'</span>)</span><br><span class="line">ap.add_argument(<span class="string">'-m'</span>,<span class="string">'--model'</span>,required=<span class="keyword">True</span>,</span><br><span class="line">                help=<span class="string">'path to output model'</span>)</span><br><span class="line">ap.add_argument(<span class="string">'-j'</span>,<span class="string">'--jobs'</span>,type=int,default=<span class="number">-1</span>,</span><br><span class="line">                help = <span class="string">'# of jobs to run when tuning hyperparameters'</span>)</span><br><span class="line">args = vars(ap.parse_args())</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>其中：</p><ul><li>db： HDF5数据集的路径</li><li>model：模型输出路径</li><li>jobs：资源使用数，默认值为-1，即使用全部资源</li></ul><p>从HDF5数据集中读取数据，并进行数据划分——75%的数据用于训练，25%用于测试:</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 读取hdf5数据集</span></span><br><span class="line">db = h5py.File(args[<span class="string">'db'</span>],<span class="string">'r'</span>)</span><br><span class="line">i = int(db[<span class="string">'labels'</span>].shape[<span class="number">0</span>] * <span class="number">0.75</span>)<span class="comment"># 分割点</span></span><br></pre></td></tr></tbody></table></figure></div><p>逻辑回归只有一个超参数需要调整——正则化系数C，我们使用网格搜索进行调参:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line">print(<span class="string">"[INFO] tuning hyperparameters..."</span>)</span><br><span class="line"><span class="comment"># 正则化系数参数范围</span></span><br><span class="line">params = {<span class="string">"C"</span>:[<span class="number">0.0001</span>,<span class="number">0.001</span>,<span class="number">0.01</span>,<span class="number">0.1</span>,<span class="number">1.0</span>]}</span><br><span class="line"><span class="comment"># 网格搜索，进行调参</span></span><br><span class="line">model = GridSearchCV(LogisticRegression(),params,cv =<span class="number">3</span>,</span><br><span class="line">                     n_jobs = args[<span class="string">'jobs'</span>])</span><br><span class="line">model.fit(db[<span class="string">'feature'</span>][:i],db[<span class="string">'labels'</span>][:i])</span><br><span class="line">print(<span class="string">'[INFO] best hyperparameters: {}'</span>.format(model.best_params_))</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>一旦我们找到C的最佳选择，我们就可以为测试集生成分类报告:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment">#性能结果</span></span><br><span class="line">print(<span class="string">'[INFO] evaluating...'</span>)</span><br><span class="line">preds = model.predict(db[<span class="string">'feature'</span>][i:])</span><br><span class="line">print(classification_report(db[<span class="string">'labels'</span>][i:],preds,</span><br><span class="line">                            target_names = db[<span class="string">'label_names'</span>]))</span><br><span class="line"><span class="comment">#计算准确度</span></span><br><span class="line">acc = accuracy_score(db[<span class="string">'labels'</span>][i:],preds)</span><br><span class="line">print(<span class="string">'[INFO] score: {}'</span>.format(acc))</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>将训练好的logistics regression模型保存到磁盘:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 保存模型</span></span><br><span class="line">print(<span class="string">'[INFO] saving model...'</span>)</span><br><span class="line"><span class="keyword">with</span> open(args[<span class="string">'model'</span>],<span class="string">'wb'</span>) <span class="keyword">as</span> fw:</span><br><span class="line">    fw.write(pickle.dumps(model.best_estimator_))</span><br><span class="line"></span><br><span class="line">db.close()</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>运行下面命令，完成整个提取特征和训练模型过程:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line">python train_model.py --db ../datasets/kaggle_dogs_vs_cats/hdf5/features.hdf5 \</span><br><span class="line">--model dogs_vs_cats.pickle</span><br><span class="line">[INFO] tuning hyperparameters...</span><br><span class="line">[INFO] best hyperparameters: {’C’: 0.001}</span><br><span class="line">[INFO] evaluating...</span><br><span class="line">precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">cat       0.99      0.99      0.99      3115</span><br><span class="line">dog       0.99      0.99      0.99      3135</span><br><span class="line"></span><br><span class="line">avg / total       0.99      0.99      0.99      6250</span><br><span class="line"></span><br><span class="line">[INFO] score: 0.986</span><br><span class="line">[INFO] saving model...</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>从输出结果中可以看到的，我们通过迁移学习的方法获得了惊人的准确率98.69%，这足以让我们在Kaggle排行榜上占据第二。</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>在本章中，我们深入研究了Kaggle猫狗数据集，并对数据集做了两组不同的实验且都获得了&gt; 90%分类准确率:</p><ul><li>1.从头训练一个AlexNet模型。</li><li>2.通过ResNet应用迁移学习。</li></ul><p>我们使用简单的AlexNet模型结构，达到了94%的分类精度。而且我们是从头开始训练一个网络，这已经是一个相当不错的结果。另外我们还可以通过一些手段提高准确度:</p><ul><li>1.获得更多的训练数据。</li><li>2.应用更高级的数据增强方法。</li><li>3.加深网络。</li></ul><p>然而，我们获得的94%精度还不足以进入前25名，更不用说前5名了。因此，为了获得前五名的位置，我们通过特征提取进行迁移学习——从ImageNet数据集训练好的ResNet50提取特征，因为ImageNet包含了许多狗和猫的例子，参考<a href="http://lonepatient.top/2018/04/02/Deep_Learning_For_Computer_Vision_With_Python_PB_08.html">第8节内容</a>，我们可以看到该挑战任务很适合使用迁移学习。正如我们的结果所显示的，我们获得98.69%的分类准确率，高到足以在排行榜上位居第二。</p><p>本文完整代码：<a href="https://github.com/lonePatient/Deep_Learning_For_Computer_Vision_With_Python/tree/master/PB" target="_blank" rel="noopener">github</a></p></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/深度学习/" rel="tag"><i class="fa fa-envira"></i> 深度学习</a> <a href="/tags/kaggle/" rel="tag"><i class="fa fa-envira"></i> kaggle</a> <a href="/tags/计算机视觉/" rel="tag"><i class="fa fa-envira"></i> 计算机视觉</a></div><div class="post-widgets"><div id="needsharebutton-postbottom"><span class="btn"><i class="fa fa-share-alt" aria-hidden="true"></i></span></div></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2018/04/22/introduce knowledge graph.html" rel="prev" title="(转)为什么需要知识图谱？什么是知识图谱？——KG的前世今生"><i class="fa fa-chevron-left"></i> (转)为什么需要知识图谱？什么是知识图谱？——KG的前世今生</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2018/04/16/IV and WOE.html" rel="next" title="数据挖掘模型中的IV和WOE详解">数据挖掘模型中的IV和WOE详解 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article>
https://lonepatient.top/2018/04/20/Deep_Learning_For_Computer_Vision_With_Python_PB_10.html
