<article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://lonepatient.top/2018/02/26/Deep_Learning_For_Computer_Vision_With_Python_PB_03.html"><span hidden="" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="eamlife"><meta itemprop="description" content=""><meta itemprop="image" content="/images/touxiang.jpeg"></span><span hidden="" itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization"><meta itemprop="name" content="eamlife's blog"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">深度学习与计算机视觉(PB-03)-特征提取</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-02-26T07:27:08+08:00">2018-02-25 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/深度学习/" itemprop="url" rel="index"><span itemprop="name">深度学习</span> </a></span>， <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/深度学习/计算机视觉/" itemprop="url" rel="index"><span itemprop="name">计算机视觉</span> </a></span></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/02/26/Deep_Learning_For_Computer_Vision_With_Python_PB_03.html#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/2018/02/26/Deep_Learning_For_Computer_Vision_With_Python_PB_03.html" itemprop="commentCount">0</span> </a></span><span id="/2018/02/26/Deep_Learning_For_Computer_Vision_With_Python_PB_03.html" class="leancloud_visitors" data-flag-title="深度学习与计算机视觉(PB-03)-特征提取"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数:</span> <span class="leancloud-visitors-count">1</span></span><div class="post-wordcount"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计:</span> <span title="字数统计">5.9k 字</span></div></div></header><div id="copyBtn" style="opacity: 0; position: absolute;top:0px;display: none;line-height: 1; font-size:1.5em"><span id="imgCopy"><i class="fa fa-paste fa-fw"></i></span><span id="imgSuccess" style="display: none;"><i class="fa fa-check-circle fa-fw" aria-hidden="true"></i></span></div><div class="post-body" itemprop="articleBody"><p>从这节开始，我们将讨论关于迁移学习的内容，即用预先训练好的模型（往往是在大型数据上训练得到的）对新的数据进行学习.</p><a id="more"></a><p>首先，从传统的机器学习场景出发，即考虑两个分类任务：</p><ul><li><p>第一个任务是训练一个卷积神经网络来识别图像中的狗和猫。</p></li><li><p>第二个任务是训练一个卷积神经网络识别三种不同的熊，即灰熊、北极熊和大熊猫。</p></li></ul><p>正常情况下，当我们使用机器学习、神经网络和深度学习等进行实践时，我们会将这两个任务视为两个独立的问题。首先，我们将收集足够多的带有标记的狗和猫的数据集，然后在该数据集上训练一个模型。一般而言，对于不同的数据，都是不断地重复这个过程，即在第二个任务中，收集足够多的带有标记的熊品种的数据集，然后在该数据集上训练一个模型。</p><p>上面两个任务中，我们独立地对每一个任务收集数据，训练模型，两个任务之间是相互独立的。而迁移学习却是另外一种不同的训练模式——假如我们加载现有预先训练好的模型，并将其作为新分类任务的训练拟合过程开始点?比如上面两个任务，首先，我们在猫和狗的数据集上训练一个卷积神经网络。然后，我们使用从猫和狗数据集中训练得到的卷积神经网络去区分熊的种类，<strong>注意的是</strong>：训练模型的数据并没有把熊数据与狗和猫的数据进行混合。</p><p>是不是听起来很神奇？两个不同任务之间可以存在关联，实际上并不是。在目前迁移学习任务中，在大型数据集(如ImageNet)上训练得到的深度神经网络是相当优秀的。这些模型学习到了一组丰富的、有区分度的特征来识别1000个不同的类别数据。因此，我们可以考虑将这些模型重新用于新的分类任务中而不用重新开始训练CNN模型。</p><p>一般来说，在计算机视觉任务中，深度学习相关的迁移学习主要有两种类型:</p><ul><li>1.将网络当作特征提取器。</li><li>2.删除现有网络的全连接层，添加新的FC层，并微调这些权重识别新的类别数据。</li></ul><p><strong>说明</strong>：第2种类型即当我们使用现有的模型进行新的分类任务时，我们控制全连接之前的模型权重不变，而利用新的数据对新的全连接层的权重进行微调（当然也可以选择微调全连接层之前的）。</p><p>在本章的后面，将演示如何使用预先训练好的CNNs(特别是VGG16)和Keras库对图像数据集(如Animals、CALTECH-101和Flowers-17)提取特征，并对特征数据训练模型。需要说明的是这三个数据集都不包含在用于训练VGG6网络的图像数据中。以往，遇到新的分类任务时，一般的流程都是重新构建模型以及对模型从头开始训练，这种方式即耗时又耗精力，但是通过转移学习，我们可以不费多大力气就可构建出高精度的图像分类器。诀窍在于特征的提取，另外，对于提取到特征，我们主要使用HDF5进行存储。</p><p>这一节，我们主要关注第一种迁移学习方法，将网络视为特征提取器。然后，我们将在<a href="">第5章</a>讨论如何根据特定的分类任务对网络的权值进行调整。</p><h2 id="CNN模型提取特征"><a href="#CNN模型提取特征" class="headerlink" title="CNN模型提取特征"></a>CNN模型提取特征</h2><p>到目前为止，我们都是把卷积神经网络当作一个端到端的图像分类器来使用，即：</p><ul><li>1.对模型输入一张图片</li><li>2.图片通过前向传播遍历整个网络</li><li>3.获得网络的分类概率结果</li></ul><p>但是，图像一定需要在整个网络中传播吗？其实，我们可以中断在任意层(如激活层或池化层)的传播，并从当前网络层中提取信息当作特征向量。例如，我们考虑VGG16网络架构(图3.1，左)。</p><p><a href="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/18-7-25/35317806.jpg" class="fancybox fancybox.image" rel="group"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/18-7-25/35317806.jpg" alt=""></a></p><center>图3.1，左：原始的VGG16网络结构图，右：移除FC层，返回pool层的结果当作特征值</center><p>上图中，除了展示VGG16网络的层结构外，还包含了每一层的输入和输出shape。如果把网络当作特征提取器，我们实际上是可以在任意点“切断”网络(通常在全连接层之前，但它实际上取决于你的特定数据集)。</p><p>在图3.1右中，我们将最大池化层当作最后一层，其中输出的shape为7×7×512，说明有512个大小为7x7的过滤器。我们可以将这些7×7×512 =25088个值当作一张图像的矢量化表征。</p><p>假如有N张图像，不断地重复这个过程，将得到一个Nx25088的矩阵，其中25088列用来表征每一张图像的内容（即我们常说的特征向量)。得到特征向量矩阵之后，我们可以利用这些特征向量训练一个现成的机器学习模型，如支持向量机、逻辑回归或随机森林等，从而获得一个新的图像识别模型。</p><p><strong>备注</strong>： 我们截断的CNN网络(如图3.1右)本身并不能识别这些新类别数据，相反，我们使用截断的CNN网络模型作为中间特征提取器，并在特征上训练一个机器学习模型来识别这些新类别数据。</p><p>下面将演示如何使用预先训练好的CNN模型(特别是VGG16)和Keras库来对图像数据集(如动物、CALTECH-101和Flowers-17)进行识别，并获得约95%的分类精度。在对图像数据提取特征之前，我们先来介绍下存储特征的工具——HDF5。</p><h3 id="HDF5"><a href="#HDF5" class="headerlink" title="HDF5"></a>HDF5</h3><p>HDF5是二进制数据格式，用于在磁盘上存储巨大的数值数据集(数据太大无法存储在内存中)，同时便于对数据集的行进行遍历和计算。HDF5中的数据是分层存储的，类似于文件系统存储数据的方式。它可以存储两类数据对象；</p><ul><li><p>1.dataset：类比于文件系统中的文件，可以操作list/ndarray的方式老操作它</p></li><li><p>2.group：类比于文件系统的文件夹，可以用操作dict的方式来操作它</p></li></ul><p>数据是定义在group中，group类似层次容器的结构，可以包含零个或多个group或dataset的实例，以及支持元数据(metadata)。一旦定义了一个group，就可以在group中创建一个dataset。dataset可以理解为是一个多维数组(例如一个NumPy数组)，<strong>注意</strong>：同一个多维数组中数据类型是相同的(整数、浮点数、unicode等)。图3.2显示了一个包含具有多个dataset的group的HDF5文件示例。本文我们将编写一个定制的Python类，使我们能够有效地接受输入数据并将其写入HDF5数据集。</p><p><a href="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/18-7-25/87672168.jpg" class="fancybox fancybox.image" rel="group"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/18-7-25/87672168.jpg" alt=""></a></p><center>图3.2</center><p>HDF5是用C编写的，但是，通过使用h5py模块(<a href="http://www.h5py.org/" target="_blank" rel="noopener">h5py.org</a>)，我们可以使用Python编程语言访问底层的C语言API。h5py之所以如此出色，是因为它易于与数据交互。我们可以在HDF5数据集中存储大量数据，并以类似于numpy的方式操作数据。例如，我们可以使用标准的Python语法访问和分割存储在磁盘上的tb级别数据集中的行，像加载到内存中操作一样简单。由于特殊的数据结构，这些分片和行访问速度非常快。当使用HDF5和h5py时，你可以将数据看作一个巨大的NumPy数组，虽然数据太大无法加载到内存中，但是仍然可以像在内存一样的访问和操作数据。</p><p>这个标准库最主要的一点在于开发者对它的积极维护以及在向下兼容方面花费的巨大精力。标准库的向下兼容不仅仅是API的兼容，亦包括文件格式的兼容，这意味着以HDF5格式存储的数据集本质上是可移植性的，可以被使用不同编程语言(如C、MATLAB和Java)的其他开发人员访问。</p><p>接下来，我们将自定义一个Python类，使我们能够有效地接受输入数据并将其写入HDF5数据集。该类的主要功能有两个：</p><ul><li><p>1.提取VGG16中的特征并以有效的方式将其写入HDF5数据集，方便我们进行迁移学习。</p></li><li><p>2.直接对原始图像生成HDF5数据集，可以加速训练速度。</p></li></ul><h3 id="使用HDF5保存特征"><a href="#使用HDF5保存特征" class="headerlink" title="使用HDF5保存特征"></a>使用HDF5保存特征</h3><p>在提取VGG16(其他CNN)网络特征之前，我们先定义存储特征的模块—HDF5DatasetWriter类，顾名思义，它负责接收一组NumPy数组(无论是特征、原始图像等)并将它们写入HDF5格式。同样，在pyimagesearch项目中创建一个新的子模块io，并在io模块中新建一个名为hdf5datasetwriter.py的文件，如下结构所示:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line">--- pyimagesearch</span><br><span class="line">|    |--- __init__.py</span><br><span class="line">|    |--- callbacks</span><br><span class="line">|    |--- io</span><br><span class="line">|        |--- __init__.py</span><br><span class="line">|        |--- hdf5datasetwriter.py</span><br><span class="line">|    |--- nn</span><br><span class="line">|    |--- preprocessing</span><br><span class="line">|    |--- utils</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>打开hdf5datasetwriter.py，写入以下代码:</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment">#encoding:utf-8</span></span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HDF5DatasetWriter</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,dims,outputPath,dataKey=<span class="string">'images'</span>,bufSize=<span class="number">1000</span>)</span>:</span></span><br><span class="line">			<span class="comment"># 判断输出路径是否存在</span></span><br><span class="line">			<span class="comment"># 当文件存在时。hdf5无法进行重写，因此进行判断处理</span></span><br><span class="line">		<span class="keyword">if</span> os.path.isfile(outputPath):</span><br><span class="line">			print(<span class="string">"The supplied ‘outputPath‘ already "</span></span><br><span class="line">				<span class="string">"exists and cannot be overwritten. Manually delete "</span></span><br><span class="line">				<span class="string">"the file before continuing."</span>, outputPath)</span><br><span class="line">			os.system(<span class="string">'rm -rf %s'</span>%outputPath)</span><br><span class="line">        <span class="comment"># 初始化一个HDF5</span></span><br><span class="line">        self.db = h5py.File(outputPath,<span class="string">'w'</span>)</span><br><span class="line">        <span class="comment"># 存储图片数据或者特征</span></span><br><span class="line">        self.data = self.db.create_dataset(dataKey,dims,dtype=<span class="string">'float'</span>)</span><br><span class="line">        <span class="comment"># 存储标签数据</span></span><br><span class="line">        self.labels = self.db.create_dataset(<span class="string">'labels'</span>,(dims[<span class="number">0</span>],),dtype=<span class="string">'int'</span>)</span><br><span class="line">        <span class="comment"># 缓冲大小</span></span><br><span class="line">        self.bufSize = bufSize</span><br><span class="line">        self.buffer = {<span class="string">"data"</span>:[],<span class="string">"labels"</span>:[]}</span><br><span class="line">        <span class="comment"># 索引</span></span><br><span class="line">        self.idx = <span class="number">0</span></span><br></pre></td></tr></tbody></table></figure></div><p>HDF5DatasetWriter类主要有四个参数，其中两个参数可选，两个参数必选。</p><ul><li>dims：数据的维度，可以将dims看作NumPy数组的.shape。</li></ul><p>如果我们存储原始的28x28=784MNIST数据集，即28x28的矩阵被拉平为(784,)向量，则dims=(70000，784)，其中70000表示样本总数，784表示每个样本的维数。如果我们想存储原始CIFAR-10图像数据，则dims=(60000,32,32,3)，即总共有60000图像样本,每个样本为32×32×3的RGB图像。</p><p>上文中提到，我们将使用VGG16网络的最后一个池化层的输出作为特征向量，从图3.1右中可以看到最后一个池化层的输出是512×7×7，拉平之后产生一个长度为25088的特征向量。因此，在使用VGG16进行特征提取时，我们将设置dims=(N, 25088)，其中N为数据集中的图像总数。</p><ul><li>outputPath：输出HDF5文件将存储在磁盘上的路径。</li><li>dataKey：用来表示数据集的名称。默认值为“images”</li></ul><p>在大多数情况下，我们将以HDF5格式存储原始图像，所以默认值为‘image’，在本例中，主要使用HDF5DatasetWriter来保存从CNN中提取到的特征，因此。这里将设设置为dataKey = “features’。</p><ul><li>bufSize：缓存的大小，默认为1000个特征向量或者原始图像，当达到bufSize时，将缓存数据写入HDF5数据集中。</li></ul><p>接下来，实现add方法，主要是将数据add到buffer中:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(self,rows,labels)</span>:</span></span><br><span class="line">    <span class="comment"># 增加数据</span></span><br><span class="line">    self.buffer[<span class="string">'data'</span>].extend(rows)</span><br><span class="line">    self.buffer[<span class="string">'labels'</span>].extend(labels)</span><br><span class="line">    <span class="comment"># 如果buffer数据大小超过bufSize，则将数据写入磁盘中</span></span><br><span class="line">    <span class="keyword">if</span> len(self.buffer[<span class="string">'data'</span>]) &gt;= self.bufSize:</span><br><span class="line">        self.flush()</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>add主要包含两个参数:</p><ul><li>rows：行数据</li><li>labels：对应的标签</li></ul><p>如果缓冲区buffer被填满，则调用flush方法将缓冲区写入文件并重新设置它们。flush方法如下：<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flush</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="comment"># 将buffers写入磁盘并初始化buffer</span></span><br><span class="line">    i = self.idx + len(self.buffer[<span class="string">'data'</span>])</span><br><span class="line">    self.data[self.idx:i] = self.buffer[<span class="string">'data'</span>]</span><br><span class="line">    self.labels[self.idx:i] = self.buffer[<span class="string">'labels'</span>]</span><br><span class="line">    self.idx = i <span class="comment"># 指针</span></span><br><span class="line">    self.buffer = {<span class="string">"data"</span>:[],<span class="string">"labels"</span>:[]}</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>把HDF5数据集看作一个大的NumPy数组，那么需要将当前索引跟踪到下一个可用的行，定位到新的数据(不重写现有数据)。</p><p>接下来定义一个函数storeClassLabels，主要在存储类标签的原始字符串名称:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">storeClassLabels</span><span class="params">(self,classLabels)</span>:</span></span><br><span class="line">    <span class="comment"># 一个dataset存储数据标签名称</span></span><br><span class="line">    dt = h5py.special_dtype(vlen = unicode)</span><br><span class="line">    labelSet =self.db.create_dataset(<span class="string">'label_names'</span>,(len(classLabels),),dtype=dt)</span><br><span class="line">    labelSet[:] = classLabels</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>最后，关闭dataset:</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">close</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(self.buffer[<span class="string">'data'</span>]) &gt;<span class="number">0</span> :</span><br><span class="line">        self.flush()</span><br><span class="line">    <span class="comment"># 关闭dataset</span></span><br><span class="line">    self.db.close()</span><br></pre></td></tr></tbody></table></figure></div><p>正如你所看到的，HDF5DatasetWriter类与机器学习和深度学习没有多大关系——它只是一个用来帮助我们以HDF5格式存储数据的类。当你不断地实践深度学习任务时，你会注意到，当建立一个新问题时，你要做的大部分工作就是把数据转换成你可以使用的格式。一旦你的数据以一种易于操作的格式进行，那么你将更加容易地将机器学习和深度学习技术应用到数据中。</p><p>因此，HDF5DatasetWriter类是一个实用类，并不是针对深度学习和计算机视觉的。现在我们的HDF5DatasetWriter已经实现，接下来我们将使用预先训练好的卷积神经网络来提取特征。</p><h2 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h2><p>定义一个Python脚本，该脚本可用于从任意图像数据集中提取特征(如果输入数据集遵循特定的目录结构)。打开一个新文件，名为extract_features.py，并写入以下代码:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment">#encoding:utf-8</span></span><br><span class="line"><span class="keyword">from</span> keras.applications <span class="keyword">import</span> VGG16</span><br><span class="line"><span class="keyword">from</span> keras.applications <span class="keyword">import</span> imagenet_utils</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.image  <span class="keyword">import</span> img_to_array</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.image <span class="keyword">import</span>  load_img</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"><span class="keyword">from</span> pyimagesearch.io <span class="keyword">import</span> hdf5datasetwriter <span class="keyword">as</span> hdf5DW</span><br><span class="line"><span class="keyword">from</span> imutils <span class="keyword">import</span> paths</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> progressbar <span class="comment"># 安装：pip install progressbar2</span></span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> os</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>其中progressbar是以个很实用的模块，提供基于文本的可视化进度条，通常用在显示下载进度、显示任务的执行进度等等。比如:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line">Extracting Features  30% |############                  | ETA: 0:00:18</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>接下来，定义命令行参数:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line">ap = argparse.ArgumentParser()</span><br><span class="line">ap.add_argument(<span class="string">"-d"</span>,<span class="string">'--dataset'</span>,required=<span class="keyword">True</span>,help=<span class="string">'path to input dataset'</span>)</span><br><span class="line">ap.add_argument(<span class="string">'-o'</span>,<span class="string">'--output'</span>,required=<span class="keyword">True</span>,help=<span class="string">'path to output HDF5 file'</span>)</span><br><span class="line">ap.add_argument(<span class="string">"-b;,'--batch_size"</span>,type =int,default=<span class="number">32</span>,help=<span class="string">'batch size of image to be passed through network'</span>)</span><br><span class="line">ap.add_argument(<span class="string">'-s'</span>,<span class="string">'--buffer_size'</span>,type=int,default=<span class="number">1000</span>,help=<span class="string">'size of feature extraction buffer'</span>)</span><br><span class="line">args = vars(ap.parse_args())</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>extract_features.py脚本总有四个命令行参数，两个必选，两个可选，其中：</p><ul><li>—dataset：输入图像数据路径</li><li>—ouput： 保存HDF5数据文件路径</li><li>—batch_size： 一次输入模型的图像个数，默认值为32，可以很据你的显存或者内存大小进行调整</li><li>—buffer_size：控制提取特征或者图像在内存中的大小</li></ul><p>接着，从磁盘中获取数据，并进行混洗以及对标签进行编码:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line">bs = args[<span class="string">'batch_size'</span>]</span><br><span class="line">print(<span class="string">"[INFO] loading image...."</span>)</span><br><span class="line"><span class="comment"># 获取所有数据的路径列表</span></span><br><span class="line">imagePaths = list(paths.list_images(args[<span class="string">'dataset'</span>]))</span><br><span class="line">random.shuffle(imagePaths)</span><br><span class="line">labels = [p.split(os.path.sep)[<span class="number">-2</span>] <span class="keyword">for</span> p <span class="keyword">in</span> imagePaths]</span><br><span class="line">le = LabelEncoder()</span><br><span class="line">labels = le.fit_transform(labels)</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>iamgePaths读取的是磁盘上所有图片的路径数据，然后进行混洗，需要说明的是由于数据太大，我们无法把所有数据都加载到内存中进行混洗，因此，采取的策略是在提取特征之前，对图像的路径进行混洗。上述从路径中提取图像标签，需要对数据的路径满足一定的结构，如下:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line">dataset_name/{class_label}/example.jpg</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>接下来，加载VGG16网络权值，并初始化HDF5DatasetWriter:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 加载 VGG16网络</span></span><br><span class="line">print(<span class="string">"[INFO] loading network....."</span>)</span><br><span class="line">model = VGG16(weights=<span class="string">'imagenet'</span>,include_top=<span class="keyword">False</span>)</span><br><span class="line"><span class="comment"># 初始化 HDF5数据写入模块</span></span><br><span class="line">dataset = hdf5DW.HDF5DatasetWriter((len(imagePaths),<span class="number">512</span>*<span class="number">7</span>*<span class="number">7</span>),args[<span class="string">'output'</span>],dataKey=<span class="string">'features'</span>,bufSize=args[<span class="string">'buffer_size'</span>])</span><br><span class="line">dataset.storeClassLabels(le.classes_)</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>首先，我们从磁盘中加载预先训练好的VGG16网络，其中需要注意的是参数include_top=False——False表明model不包含最后三个全连接层。因此，当对模型输入一张图像时，我们将获得最后一个池化层的特征值，而不是FC层中softmax分类器产生的概率。</p><p><strong>备注</strong>： 若磁盘不存在VGG16网络，则脚本会自动下载，模型保存的路径主要在；～/.keras/model/中</p><p>接下来进行特征提取：<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 初始化进度条</span></span><br><span class="line">widgets = [<span class="string">'EXtracting Features: '</span>,progressbar.Percentage(),<span class="string">" "</span>,progressbar.Bar(),<span class="string">' '</span>,progressbar.ETA()]</span><br><span class="line">pbar = progressbar.ProgressBar(maxval=len(imagePaths),widgets=widgets).start()</span><br><span class="line"><span class="comment"># 每batch_size遍历全量图像数据</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> np.arange(<span class="number">0</span>,len(imagePaths),bs):</span><br><span class="line">    <span class="comment"># 提取batch_size的图像以及标签数据</span></span><br><span class="line">    batchPaths = imagePaths[i:i+bs]</span><br><span class="line">    batchLabels = labels[i:i+bs]</span><br><span class="line">    batchIMages = []</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>初始化进度条，这样可以查看整个数据集提取特征的进度信息。下面进行图像数据预处理：<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="keyword">for</span> (j,imagePath) <span class="keyword">in</span> enumerate(batchPaths):</span><br><span class="line">    <span class="comment"># 加载图像，并调整大小为224x224</span></span><br><span class="line">    image = load_img(imagePath,target_size=(<span class="number">224</span>,<span class="number">224</span>))</span><br><span class="line">    image = img_to_array(image)</span><br><span class="line">    <span class="comment"># 图像预处理</span></span><br><span class="line">    <span class="comment"># 1. 增加一个维度</span></span><br><span class="line">    <span class="comment"># 2. 利用imagenet信息进行标准化处理</span></span><br><span class="line">    image = np.expand_dims(image,axis =<span class="number">0</span>)</span><br><span class="line">    image = imagenet_utils.preprocess_input(image)</span><br><span class="line">    <span class="comment"># 将处理玩的图像加入batch</span></span><br><span class="line">    batchImages.append(image)</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>图像预处理跟上节内容差不多，主要是新增一个维度和标准化。<br><strong>注意</strong>： 这里的标准化，需要调用预训练VGG16模型的数据进行标准化，保证数据一致性。</p><p>保存特征：<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 使用模型的预测值作为特征向量</span></span><br><span class="line">      batchImages = np.vstack(batchImages)</span><br><span class="line">      features = model.predict(batchImages,batch_size=bs)</span><br><span class="line">        <span class="comment"># 将最后一个池化层拉平，调整特征的大小</span></span><br><span class="line">      features = features.reshape((features.shape[<span class="number">0</span>],<span class="number">512</span> * <span class="number">7</span>* <span class="number">7</span>))</span><br><span class="line">        <span class="comment"># 将得到的特征和标签加入HDF5数据集中</span></span><br><span class="line">      dataset.add(features,batchLabels)</span><br><span class="line">      pbar.update(i)</span><br><span class="line">dataset.close()</span><br><span class="line">pbar.finish()</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>我们使用numpy.vstack方法来“垂直叠加”图像数据，使它们具有形状(N, 224, 224, 3)，其中N是batch的大小。</p><p>调用model.predict方法得到batch大小的特征向量集合——<strong>注意</strong>，我们删除了VGG16的全连接层，所以得到的特征向量是最后一个最大池化层输出的值。但是，最大池化层输出的形状是(N,512 7，7)，这意味着有512个大小7×7的过滤器。要将这些值转化为特征向量，我们需要将它们拉平成一个具有形状(N, 25088)的数组，即reshape((features.shape[0],512 <em>7</em> 7))。</p><p>提取特征代码完成之后，我们将使用预先训练好的CNN模型从各种数据集中提取特征。</p><h3 id="提取特征"><a href="#提取特征" class="headerlink" title="提取特征"></a>提取特征</h3><p>首先，我们将使用VGG6模型对“animals”数据集提取特征，该数据集由3000张图片组成，包括三个类别:狗、猫和熊猫。执行以下命令，以获取特征向量:</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="meta">$</span><span class="bash"> python extract_features.py --dataset youPath/data/animals/images \</span></span><br><span class="line">--output youPath/data/animals/hdf5/features.hdf5</span><br></pre></td></tr></tbody></table></figure></div><p>执行玩之后，在animals/hdf5目录，您将看到一个名为features.hdf5的文件:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="meta">$</span><span class="bash"> ls youpath/data/animals/hdf5/</span></span><br><span class="line">features.hdf5</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>新开一个python shell，用来查看.hdf5数据的信息：<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="meta">$</span><span class="bash"> python</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; import h5py</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; p = <span class="string">"youPath/data/animals/hdf5/features.hdf5"</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; db = h5py.File(p)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; list(db.keys())</span></span><br><span class="line">[u’features’, u’label_names’, u’labels’]</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; db[<span class="string">"features"</span>].shape</span></span><br><span class="line">(3000, 25088)</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; db[<span class="string">"labels"</span>].shape</span></span><br><span class="line">(3000,)</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; db[<span class="string">"label_names"</span>].shape</span></span><br><span class="line">(3,)</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>可以看到HDF5数据文件中包含三个dataset: features、label_names、labels。</p><h3 id="从CALTECH-101中提取特征"><a href="#从CALTECH-101中提取特征" class="headerlink" title="从CALTECH-101中提取特征"></a>从CALTECH-101中提取特征</h3><p>与前面一样，运行下面命令，就可以得到CALTECH-101数据的特征，我们只需要修改数据的路径即可，：<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="meta">$</span><span class="bash"> python extract_features.py --dataset youPath/data/caltech-101/images \</span></span><br><span class="line">--output youPath/data/caltech-101/hdf5/features.hdf5</span><br></pre></td></tr></tbody></table></figure></div><p></p><h3 id="从Flowers-17中提取特征"><a href="#从Flowers-17中提取特征" class="headerlink" title="从Flowers-17中提取特征"></a>从Flowers-17中提取特征</h3><p>运行下面命令进行特征提取:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="meta">$</span><span class="bash"> python extract_features.py --dataset youPath/data/flowers17/images \</span></span><br><span class="line">--output youPath/data/flowers17/hdf5/features.hdf5</span><br></pre></td></tr></tbody></table></figure></div><p></p><h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><p>上面我们使用了预训练好的VGG16模型对animals、CALTECH-101和FLowers-17S数据集提取了特征。接下来，我们将对这些特征数据构建一个机器学习模型进行分类。</p><p>新建一个脚本，名为train_model.py，并写入下面代码：<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment">#encoding:utf-8</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span>  LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV <span class="comment"># 用于模型进行调参处理</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report <span class="comment"># 模型评估结果</span></span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> h5py</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>首先，加载所需要的模块，一开始我们使用简单的逻辑回归模型进行分类。</p><p>命令行解析参数；<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line">ap = argparse.ArgumentParser()</span><br><span class="line">ap.add_argument(<span class="string">"-d"</span>,<span class="string">"--db"</span>,required = <span class="keyword">True</span>,help = <span class="string">'path HDF5 database'</span>)</span><br><span class="line">ap.add_argument(<span class="string">"-m"</span>,<span class="string">"--model"</span>,required = <span class="keyword">True</span>,help = <span class="string">'path to output model'</span>)</span><br><span class="line">ap.add_argument(<span class="string">'-j'</span>,<span class="string">'--jobs'</span>,type=int,default=<span class="number">-1</span>,help=<span class="string">'# of jobs to run when tuning hyperparameters'</span>)</span><br><span class="line">args = vars(ap.parse_args())</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>其中：</p><ul><li>—db: hdf5数据的路径</li><li>—model: 保存模型路径</li><li>—jobs：模型调优时的并发数，默认-1值表示使用全部资源数</li></ul><p>将数据划分为train和test数据:</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line">db = h5py.File(args[<span class="string">'db'</span>],<span class="string">'r'</span>)</span><br><span class="line">i = int(db[<span class="string">'labels'</span>].shape[<span class="number">0</span>] * <span class="number">0.75</span>)</span><br></pre></td></tr></tbody></table></figure></div><p>可以看到这里的划分数据跟之前使用train_test_split不一样，主要是数据太大，我们无法加载到内存中进行混洗。前面提到过在将相关的图像/特征向量写入HDF5数据集之前，我们对图像路径数据进行了混洗了——从而我们直接利用第二行代码划分数据即可。混洗完之后将使用前75%的数据作为train数据集，后25%的数据作为test数据集。</p><p>下面，我们将加载模型；<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 模型调优</span></span><br><span class="line">print(<span class="string">"[INFO] tuning hyperparameters..."</span>)</span><br><span class="line"><span class="comment"># 逻辑逻辑模型参数不多，只有一个惩罚项系数</span></span><br><span class="line">params = {<span class="string">"C"</span>:[<span class="number">0.1</span>,<span class="number">1.0</span>,<span class="number">10.0</span>,<span class="number">100.0</span>,<span class="number">1000.0</span>,<span class="number">10000.0</span>]}</span><br><span class="line">model = GridSearchCV(LogisticRegression(),params,cv=<span class="number">3</span>,n_jobs=args[<span class="string">'jobs'</span>])</span><br><span class="line">model.fit(db[<span class="string">'features'</span>][:i],db[<span class="string">'labels'</span>][:i])</span><br><span class="line">print(<span class="string">"[INFO] best hyperparameters :{} "</span>.format(model.best_estimator_))</span><br><span class="line"><span class="comment"># 评估模型</span></span><br><span class="line">print(<span class="string">"[INFO] evaluating..."</span>)</span><br><span class="line">preds = model.predict(db[<span class="string">'features'</span>][i:])</span><br><span class="line">print(classification_report(db[<span class="string">'labels'</span>][i:],preds,target_names=db[<span class="string">'label_names'</span>]))</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>将模型保存到磁盘中，方便下次调取使用:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 保存模型到磁盘</span></span><br><span class="line">print(<span class="string">"[INFO] saving model..."</span>)</span><br><span class="line">f = open(args[<span class="string">'model'</span>],<span class="string">'wb'</span>)</span><br><span class="line">f.write(pickle.dumps(model.best_estimator_))</span><br><span class="line">f.close()</span><br><span class="line">db.close()</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>完成模型训练部分代码之后，我们将对上面提取好的特征数据进行训练，并查看结果.</p><h3 id="Animals数据结果"><a href="#Animals数据结果" class="headerlink" title="Animals数据结果"></a>Animals数据结果</h3><p>对Animals提取到的特征数据进行模型训练，运行下面命令；<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line">python train_model.py --db youPath/data/animals/hdf5/features.hdf5 \</span><br><span class="line">--model animals.cpickle</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>将得到如下结果:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line">             precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">       cats       0.98      0.99      0.99       252</span><br><span class="line">       dogs       0.99      0.98      0.99       245</span><br><span class="line">      panda       1.00      1.00      1.00       253</span><br><span class="line"></span><br><span class="line">avg / total       0.99      0.99      0.99       750</span><br></pre></td></tr></tbody></table></figure></div><p></p><h3 id="CALTECH-101的结果"><a href="#CALTECH-101的结果" class="headerlink" title="CALTECH-101的结果"></a>CALTECH-101的结果</h3><p>运行下面命令:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line">python train_model.py \</span><br><span class="line">--db youPath/data/caltech-101/hdf5/features.hdf5 \</span><br><span class="line">--model caltech101.cpickle</span><br></pre></td></tr></tbody></table></figure></div><p></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这一章中，我们讨论了迁移学习，主要使用预先训练好的卷积神经网络对新的类别数据进行分类。一般来说，在计算机视觉任务中，深度学习相关的迁移学习主要有两种类型:</p><ul><li>1.将网络当作特征提取器。</li><li>2.删除现有网络的全连接层，并添加新的FC层，并微调这些权重识别新的类别数据</li></ul><p>这节我们主要集中讨论关于特征提取内容，通过例子证明了卷积神经网络，比如VGG,Inception和ResNet，能够作为强大的特征提取模型，甚至比手工提取特征方法，比如HOG[14]，SIFT[15]和Local Binary Patterns[16]等更加有效。当遇到深度学习特别是与卷积神经网络相关的问题时，可以考虑应用特征提取，然后训练机器学习算法，看看是否能够获得合理的准确度——如果可以，那么我们完全可以跳过网络训练过程，从而节省大量的时间和精力。</p><p>本文数据下载地址: <a href="https://drive.google.com/open?id=1y8LMhd9sphL3Kd1obKIlgFhbU79dgRhR" target="_blank" rel="noopener">googleDrive</a><br>本文完整的代码地址: <a href="https://github.com/lonePatient/Deep_Learning_For_Computer_Vision_With_Python/tree/master/PB" target="_blank" rel="noopener">github</a></p></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/深度学习/" rel="tag"><i class="fa fa-envira"></i> 深度学习</a> <a href="/tags/计算机视觉/" rel="tag"><i class="fa fa-envira"></i> 计算机视觉</a></div><div class="post-widgets"><div id="needsharebutton-postbottom"><span class="btn"><i class="fa fa-share-alt" aria-hidden="true"></i></span></div></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2018/02/27/understand-convolution.html" rel="prev" title="对深度可分离卷积、分组卷积、扩张卷积、转置卷积（反卷积）的理解"><i class="fa fa-chevron-left"></i> 对深度可分离卷积、分组卷积、扩张卷积、转置卷积（反卷积）的理解</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2018/02/25/Gradient Boosting Machine_tuning.html" rel="next" title="Gradient Boosting Machine(GBM）调参方法详解">Gradient Boosting Machine(GBM）调参方法详解 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article>
https://lonepatient.top/2018/02/26/Deep_Learning_For_Computer_Vision_With_Python_PB_03.html
