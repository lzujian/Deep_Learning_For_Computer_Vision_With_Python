<article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://lonepatient.top/2018/04/10/Deep_Learning_For_Computer_Vision_With_Python_PB_09.html"><span hidden="" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="eamlife"><meta itemprop="description" content=""><meta itemprop="image" content="/images/touxiang.jpeg"></span><span hidden="" itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization"><meta itemprop="name" content="eamlife's blog"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">深度学习与计算机视觉(PB-09)-使用HDF5保存大数据集</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-04-10T07:27:08+08:00">2018-04-09 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/深度学习/" itemprop="url" rel="index"><span itemprop="name">深度学习</span> </a></span>， <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/深度学习/计算机视觉/" itemprop="url" rel="index"><span itemprop="name">计算机视觉</span> </a></span></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/04/10/Deep_Learning_For_Computer_Vision_With_Python_PB_09.html#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/2018/04/10/Deep_Learning_For_Computer_Vision_With_Python_PB_09.html" itemprop="commentCount">0</span> </a></span><span id="/2018/04/10/Deep_Learning_For_Computer_Vision_With_Python_PB_09.html" class="leancloud_visitors" data-flag-title="深度学习与计算机视觉(PB-09)-使用HDF5保存大数据集"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数:</span> <span class="leancloud-visitors-count">1</span></span><div class="post-wordcount"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计:</span> <span title="字数统计">2.8k 字</span></div></div></header><div id="copyBtn" style="opacity: 0; position: absolute;top:0px;display: none;line-height: 1; font-size:1.5em"><span id="imgCopy"><i class="fa fa-paste fa-fw"></i></span><span id="imgSuccess" style="display: none;"><i class="fa fa-check-circle fa-fw" aria-hidden="true"></i></span></div><div class="post-body" itemprop="articleBody"><p>到目前为止，我们使用的数据集都能够全部加载到内存中。对于小数据集，我们可以加载全部图像数据到内存中，进行预处理，并进行前向传播处理。然而，对于大规模数据集(比如ImageNet),我们需要创建数据生成器，每次只访问一小部分数据集（比如mini-batch），然后对batch数据进行预处理和前向传播。</p><a id="more"></a><p>Keras模块很方便进行数据加载，可以使用磁盘上的原始文件路径作为训练过程的输入。你不需要将整个数据集存储在内存中——只需为Keras数据生成器提供图像路径，生成器会自动从路径中加载数据并进行前向传播。</p><p>然而，这种方法非常低效。读取磁盘上的每张图像都需要一个I/O操作，这样会造成一定的延迟。训练深度学习网络本身已经够慢了，所以我们应该尽可能避免I/O瓶颈。</p><p>一个比较合理的解决方案是将原始图像生成HDF5数据集，就像我们<a href="http://lonepatient.top/2018/02/25/Deep_Learning_For_Computer_Vision_With_Python_PB_03.html">第3章</a>中所做的那样，只是这一次我们存储的是原始图像，而不是提取的特征。HDF5不仅可以存储大量的数据集，而且还可以用于I/O操作，特别是用于从文件中提取batch(称为“片”)。我们将在磁盘上的原始图像保存到HDF5文件中，这可以让模型快速的遍历数据集并在其上训练深度学习网络。</p><p>在本章的其余部分，将演示如何为Kaggle上的猫狗比赛[3]构建一个HDF5数据集。然后，在下一章中，我们将使用这个HDF5数据集来训练具有开创性的AlexNet架构[6]，并在排行榜中排名前25。</p><h2 id="Kaggle-猫狗比赛"><a href="#Kaggle-猫狗比赛" class="headerlink" title="Kaggle: 猫狗比赛"></a>Kaggle: 猫狗比赛</h2><p>首先，从<a href="http://pyimg.co/xb5lb" target="_blank" rel="noopener">kaggle官方网页地址</a>上下载狗和猫的数据集.</p><p>下载完数据之后，按照下面目录结构解压数据：<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line">kaggle_dogs_vs_cats/train/cat.11866.jpg</span><br><span class="line">...</span><br><span class="line">kaggle_dogs_vs_cats/train/dog.11046.jpg</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>这个项目中将使用以下数据结构:</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line">|--- datasets</span><br><span class="line">| |--- kaggle_dogs_vs_cats</span><br><span class="line">| | |--- hdf5</span><br><span class="line">| | |--- train</span><br><span class="line">|--- dogs_vs_cats</span><br><span class="line">| |--- config</span><br><span class="line">| |--- build_dogs_vs_cats.py</span><br><span class="line">| |--- ...</span><br></pre></td></tr></tbody></table></figure></div><p>接下来，我们创建配置文件。</p><h2 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h2><p>现在我们开始构建更高级的项目和深度学习算法，为了好管理，将使用python为每个项目创建一个特殊的配置模块。例如，以下是kagge猫狗项目的目录结构:</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line">--- dogs_vs_cats</span><br><span class="line">| |--- config</span><br><span class="line">| | |--- __init__.py</span><br><span class="line">| | |--- dogs_vs_cats_config.py</span><br><span class="line">| |--- build_dogs_vs_cats.py</span><br><span class="line">| |--- crop_accuracy.py</span><br><span class="line">| |--- extract_features.py</span><br><span class="line">| |--- train_alexnet.py</span><br><span class="line">| |--- train_model.py</span><br><span class="line">| |--- output/</span><br><span class="line">| | |--- __init__.py</span><br><span class="line">| | |--- alexnet_dogs_vs_cats.model</span><br><span class="line">| | |--- dogs_vs_cats_features.hdf5</span><br><span class="line">| | |--- dogs_vs_cats_mean.json</span><br><span class="line">| | |--- dogs_vs_cats.pickle</span><br></pre></td></tr></tbody></table></figure></div><p>在config目录中新建一个名为dogs_vs_cats_config.py文件，该文件的主要目的是存储项目的所有相关配置，包括:</p><ul><li><p>1.输入图像的路径。</p></li><li><p>2.类标签的总数。</p></li><li><p>3.训练、验证和测试数据划分信息。</p></li><li><p>4.HDF5数据集的路径。</p></li><li><p>5.模型、图表和日志等保存路径。</p></li></ul><p>配置文件使用Python格式而不是JSON格式，主要是为了使配置文件更有效地使用(可以使用os.path模块进行操作文件路径)。我建议您养成使用python脚本作为自己的深度学习项目的配置文件的习惯，因为它将极大地提高您的工作效率，并允许您通过单个文件控制项目中的大部分参数。</p><h3 id="配置信息"><a href="#配置信息" class="headerlink" title="配置信息"></a>配置信息</h3><p>打开dogs_vs_cats_config.py，并写入以下代码:</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 原始图像路径</span></span><br><span class="line">IMAGES_PATH = <span class="string">"../datasets/kaggle_dogs_vs_cats/train"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#类别总数</span></span><br><span class="line">NUM_CLASSES = <span class="number">2</span></span><br><span class="line"><span class="comment"># 验证数据集大小</span></span><br><span class="line">NUM_VAL_IMAGES = <span class="number">1250</span> * NUM_CLASSES</span><br><span class="line"><span class="comment"># 测试数据集代销</span></span><br><span class="line">NUM_TEST_IMAGES = <span class="number">1250</span> * NUM_CLASSES</span><br><span class="line"></span><br><span class="line"><span class="comment"># hdf5数据保存路径</span></span><br><span class="line">TRAIN_HDF5 = <span class="string">"../datasets/kaggle_dogs_vs_cats/hdf5/train.hdf5"</span></span><br><span class="line">VAL_HDF5 = <span class="string">"../datasets/kaggle_dogs_vs_cats/hdf5/val.hdf5"</span></span><br><span class="line">TEST_HDF5 = <span class="string">"../datasets/kaggle_dogs_vs_cats/hdf5/test.hdf5"</span></span><br></pre></td></tr></tbody></table></figure></div><p>下面定义输出序列化权重的路径、数据集平均值和图、分类报告、日志等的输出保存路径:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 模型保存路径</span></span><br><span class="line">MODEL_PATH = <span class="string">"output/alexnet_dogs_vs_cats.model"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据均值保存路径</span></span><br><span class="line">DATASET_MEAN = <span class="string">"output/dogs_vs_cats_mean.json"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 其余输出保存路径</span></span><br><span class="line">OUTPUT_PATH = <span class="string">"output"</span></span><br></pre></td></tr></tbody></table></figure></div><p></p><p>DATASET_MEAN文件主要是存储整个(training)数据集中三个颜色通道（红、绿、蓝）的像素平均值。在训练网络之前，我们需要将图像中的每个像素值减去平均值(验证集和测试集也需要进行操作处理)。这种方法称为零均值化，是一种数据归一化技术，它将像素强度归一化到[0，1]范围中，这对大型数据集和更深层次的神经网络的训练很有效。</p><h2 id="保存数据"><a href="#保存数据" class="headerlink" title="保存数据"></a>保存数据</h2><p>下面，构建HDF5数据集。打开一个新文件，命名为build_dogs_vs_cats.py，写入以下代码:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载模块</span></span><br><span class="line"><span class="keyword">from</span> config <span class="keyword">import</span> dogs_vs_cats_config <span class="keyword">as</span> config</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> pyimagesearch.preprocessing <span class="keyword">import</span> AspectAwarePreprocessor <span class="keyword">as</span> AAP</span><br><span class="line"><span class="keyword">from</span> pyimagesearch.io <span class="keyword">import</span> hdf5datasetwriter <span class="keyword">as</span> HDF</span><br><span class="line"><span class="keyword">from</span> imutils <span class="keyword">import</span> paths</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> progressbar</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> os</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>获取Kaggle猫狗数据集中的图像路径:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 图像路径</span></span><br><span class="line">trainPaths = list(paths.list_images(config.IMAGES_PATH))</span><br><span class="line"><span class="comment"># 获取标签</span></span><br><span class="line">trainLabels = [p.split(os.path.sep)[<span class="number">2</span>].split(<span class="string">"."</span>)[<span class="number">0</span>] <span class="keyword">for</span> p <span class="keyword">in</span> trainPaths]</span><br><span class="line"><span class="comment"># 标签编码化</span></span><br><span class="line">le = LabelEncoder()</span><br><span class="line">trainLabels = le.fit_transform(trainLabels)</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>原始的猫狗数据集的目录结构如下:</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line">kaggle_dogs_vs_cats/train/cat.11866.jpg</span><br><span class="line">...</span><br><span class="line">kaggle_dogs_vs_cats/train/dog.11046.jpg</span><br></pre></td></tr></tbody></table></figure></div><p><strong>注意</strong>：类的名称是从实际的文件名中获取到的。</p><p>接下来，我们将原始数据分成三部分:训练集、验证集和测试集。</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 将原始的train分割成train和test两份</span></span><br><span class="line">split = train_test_split(trainPaths, trainLabels,test_size=config.NUM_TEST_IMAGES, stratify=trainLabels,random_state=<span class="number">42</span>)</span><br><span class="line">(trainPaths, testPaths, trainLabels, testLabels) = split</span><br><span class="line"><span class="comment">#　将新的train分割成train和val两份</span></span><br><span class="line">split = train_test_split(trainPaths, trainLabels,test_size=config.NUM_VAL_IMAGES, stratify=trainLabels,random_state=<span class="number">42</span>)</span><br><span class="line">(trainPaths, valPaths, trainLabels, valLabels) = split</span><br></pre></td></tr></tbody></table></figure></div><p>划分完数据集之后，我们将数据集写入HDF5文件中:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 将数据构建一个list，方便写入HDF5文件中</span></span><br><span class="line">datasets = [</span><br><span class="line"> (<span class="string">"train"</span>, trainPaths, trainLabels, config.TRAIN_HDF5),</span><br><span class="line"> (<span class="string">"val"</span>, valPaths, valLabels, config.VAL_HDF5),</span><br><span class="line"> (<span class="string">"test"</span>, testPaths, testLabels, config.TEST_HDF5)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据预处理</span></span><br><span class="line">aap = AAP.AspectAwarePreprocessor(<span class="number">256</span>, <span class="number">256</span>)</span><br><span class="line">(R, G, B) = ([], [], [])</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>首先，我们定义一个数据集列表，其中包括训练数据集、验证数据集和测试数据集以及各自相关变量。列表中的每个元素都是一个4元组，包括:</p><ul><li><p>1.数据类型名称。</p></li><li><p>2.数据路径。</p></li><li><p>3.数据标签。</p></li><li><p>4.保存HDF5数据的路径。</p></li></ul><p>然后，我们初始化AspectAwarePreprocessor，在写入HDF5之前将图像大小调整为256x256像素(注意，图像的纵横比)。最后，初始化三个列表——R、G和B（图像的三个颜色通道），用于存储每个通道的平均像素值。</p><p>最后，我们准备构建HDF5数据集:</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 遍历数据集</span></span><br><span class="line"><span class="keyword">for</span> (dType, paths, labels, outputPath) <span class="keyword">in</span> datasets:</span><br><span class="line">    <span class="comment"># HDF5 writer</span></span><br><span class="line">    print(<span class="string">"[INFO] building {}..."</span>.format(outputPath))</span><br><span class="line">    writer = HDF.HDF5DatasetWriter((len(paths), <span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>), outputPath)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化进度条</span></span><br><span class="line">    widgets = [<span class="string">"Building Dataset: "</span>, progressbar.Percentage(), <span class="string">" "</span>,</span><br><span class="line">    progressbar.Bar(), <span class="string">" "</span>, progressbar.ETA()]</span><br><span class="line">    pbar = progressbar.ProgressBar(maxval=len(paths),</span><br><span class="line">    widgets=widgets).start()</span><br></pre></td></tr></tbody></table></figure></div><p>其中HDF5DatasetWriter的输出数据集的shape是(len(paths),256,256,3)，这意味着有len(paths)张图像且每一个shape都是(256,256,3)</p><p>然后初始化进度条模块，这样我们就可以很容易地监视数据集生成的过程。</p><p>接下来，写入数据:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="keyword">for</span> (i, (path, label)) <span class="keyword">in</span> enumerate(zip(paths, labels)):</span><br><span class="line">    <span class="comment"># 读取数据并预处理</span></span><br><span class="line">    image = cv2.imread(path)</span><br><span class="line">    image = aap.preprocess(image)</span><br><span class="line">    <span class="comment"># 如果是train，则计算RGB均值</span></span><br><span class="line">    <span class="keyword">if</span> dType == <span class="string">"train"</span>:</span><br><span class="line">        (b, g, r) = cv2.mean(image)[:<span class="number">3</span>]</span><br><span class="line">        R.append(r)</span><br><span class="line">        G.append(g)</span><br><span class="line">        B.append(b)</span><br><span class="line">    <span class="comment"># 写入数据</span></span><br><span class="line">    writer.add([image], [label])</span><br><span class="line">    pbar.update(i)</span><br><span class="line"></span><br><span class="line">pbar.finish()</span><br><span class="line">writer.close()</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>首先，对图像路径数据进行遍历，读取每一张图像，并进行预处理，将图像的大小调整为256x256。</p><p>然后，如果是train数据集，则我们计算RGB的平均值（需要注意的是：我们只在train数据中计算RGB平均值，而验证集和测试集直接减去平均值）。</p><p>最后，我们将经过处理的图像写入HDF5文件中。</p><p>将RGB平均值序列化到磁盘:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 保存成json文件</span></span><br><span class="line">print(<span class="string">"[INFO] serializing means..."</span>)</span><br><span class="line">D = {<span class="string">"R"</span>: np.mean(R), <span class="string">"G"</span>: np.mean(G), <span class="string">"B"</span>: np.mean(B)}</span><br><span class="line">f = open(config.DATASET_MEAN, <span class="string">"w"</span>)</span><br><span class="line">f.write(json.dumps(D))</span><br><span class="line">f.close()</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>打开终端，直接以下命令：<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="meta">$</span> python build_dogs_vs_cats.py</span><br><span class="line">[INFO] building kaggle_dogs_vs_cats/hdf5/train.hdf5...</span><br><span class="line">Building Dataset: 100% |####################################| Time: 0:02:39</span><br><span class="line">[INFO] building kaggle_dogs_vs_cats/hdf5/val.hdf5...</span><br><span class="line">Building Dataset: 100% |####################################| Time: 0:00:20</span><br><span class="line">[INFO] building kaggle_dogs_vs_cats/hdf5/test.hdf5...</span><br><span class="line">Building Dataset: 100% |####################################| Time: 0:00:19</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>从输出结果中可以看到，训练数据集、测试数据集和验证数据集分别创建了一个HDF5文件。</p><p>查看这些文件大小<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="meta">$</span> ls -l ../datasets/kaggle_dogs_vs_cats/hdf5/</span><br><span class="line">total 38400220</span><br><span class="line">-rw-rw-r-- 1 adrian adrian 3932182144 Apr 7 18:00 test.hdf5</span><br><span class="line">-rw-rw-r-- 1 adrian adrian 31457442144 Apr 7 17:59 train.hdf5</span><br><span class="line">-rw-rw-r-- 1 adrian adrian 3932182144 Apr 7 18:00 val.hdf5</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>磁盘上的猫狗数据集的大小只有595MB，为什么.hdf5文件这么大?train.hdf5文件是31.45GB，val.hdf5和test.hdf5文件几乎是4GB的.</p><p>原始的图像文件格式如JPEG和PNG应用了数据压缩算法来压缩了图像文件的大小。但是，实际上我们读取图像数据时，没有对数据进行任何压缩，保持了原本图像的大小，并将图像存储为原始的NumPy数组(比如,bitmaps)，极大地增加了我们的存储成本，但也将加快我们的训练速度，因为我们可以直接从HDF5数据集读取图像，并进行预处理和前向传播。</p><p>查看RGB均值文件:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="meta">$</span> cat output/dogs_vs_cats_mean.json</span><br><span class="line">{"B": 106.13178224639893, "R": 124.96761639328003, "G": 115.97504255599975}</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>其中，红色通道在数据集中的所有图像中平均像素强度为124.96。蓝色通道的平均值是106.13，绿色通道的平均值是115.97。下一章中，我们将会对输入图像的像素值减去这些RGB平均值，即对图像数据做归一化。均值归一化有助于将数据集中在0均值附近。通常，这种规范化使我们的网络能够更快地学习，这也是为什么我们在更大、更有挑战性的数据集中使用这种类型的规范化(而不是[0，1]缩放)。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在本章中，我们学习了如何将原始图像序列化为适合训练深度神经网络的HDF5数据集。我们将原始图像序列化为HDF5文件，而不是简单地访问磁盘上的一小批图像路径，由于会造成I/O延迟——对于磁盘上的每张图像，我们必须执行一次I/O操作来读取图像。在深度训练工程中，I/O延迟是一个大问题——训练过程已经足够缓慢,如果我们的网络很难访问数据,那么就是搬起石头砸自己的脚。</p><p>相反，如果我们将所有图像序列化到一个高效的HDF5文件中，我们可以利用非常快速的数组切片来提取我们的mini-batch数据，从而显著减少I/O延迟，并且加快训练过程。无论何时，当您使用Keras库并处理一个太大而无法装入内存的数据集时，您首先应该考虑将数据集序列化为HDF5格式——我们将在下一章中发现，它使训练您的网络成为一项更容易(也更有效)的任务。</p><p>本章代码下载地址:<a href="https://github.com/lonePatient/Deep_Learning_For_Computer_Vision_With_Python/upload/master/PB" target="_blank" rel="noopener">github</a></p></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/深度学习/" rel="tag"><i class="fa fa-envira"></i> 深度学习</a> <a href="/tags/计算机视觉/" rel="tag"><i class="fa fa-envira"></i> 计算机视觉</a></div><div class="post-widgets"><div id="needsharebutton-postbottom"><span class="btn"><i class="fa fa-share-alt" aria-hidden="true"></i></span></div></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2018/04/13/Face_clustering_with_Python.html" rel="prev" title="人脸聚类——python实现"><i class="fa fa-chevron-left"></i> 人脸聚类——python实现</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2018/04/07/Porto-Seguros-Safe-Driver-Prediction.html" rel="next" title="Porto_Seguros Safe Driver Prediction">Porto_Seguros Safe Driver Prediction <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article>
https://lonepatient.top/2018/04/10/Deep_Learning_For_Computer_Vision_With_Python_PB_09.html
