<div class="post-block"><link itemprop="mainEntityOfPage" href="http://lonepatient.top/2018/03/10/Deep_Learning_For_Computer_Vision_With_Python_PB_05.html"><span hidden="" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="eamlife"><meta itemprop="description" content=""><meta itemprop="image" content="/images/touxiang.jpeg"></span><span hidden="" itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization"><meta itemprop="name" content="eamlife's blog"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">深度学习与计算机视觉(PB-05)-网络微调</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-10T07:27:08+08:00">2018-03-09 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/深度学习/" itemprop="url" rel="index"><span itemprop="name">深度学习</span> </a></span>， <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/深度学习/计算机视觉/" itemprop="url" rel="index"><span itemprop="name">计算机视觉</span> </a></span></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/03/10/Deep_Learning_For_Computer_Vision_With_Python_PB_05.html#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/2018/03/10/Deep_Learning_For_Computer_Vision_With_Python_PB_05.html" itemprop="commentCount">0</span> </a></span><span id="/2018/03/10/Deep_Learning_For_Computer_Vision_With_Python_PB_05.html" class="leancloud_visitors" data-flag-title="深度学习与计算机视觉(PB-05)-网络微调"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数:</span> <span class="leancloud-visitors-count">3</span></span><div class="post-wordcount"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计:</span> <span title="字数统计">4.8k 字</span></div></div></header><div id="copyBtn" style="opacity: 0; position: absolute;top:0px;display: none;line-height: 1; font-size:1.5em"><span id="imgCopy"><i class="fa fa-paste fa-fw"></i></span><span id="imgSuccess" style="display: none;"><i class="fa fa-check-circle fa-fw" aria-hidden="true"></i></span></div><div class="post-body" itemprop="articleBody"><p>在<a href="http://lonepatient.top/2018/02/25/Deep_Learning_For_Computer_Vision_With_Python_PB_03.html">第3节</a>中，我们学习了如何将预训练好的卷积神经网络作为特征提取器。通过加载预训练好的模型，可以提取指定层的输出作为特征向量，并将特征向量保存到磁盘。有了特征向量之后，我们就可以在特征向量上训练传统的机器学习算法(比如在<a href="http://lonepatient.top/2018/02/25/Deep_Learning_For_Computer_Vision_With_Python_PB_03.html">第3节</a>中我们使用的逻辑回归模型)。当然对于特征向量，我们也可以使用手工提取特征方法，比如SIFT[15],HOG[14],LBPs[16]等。</p><a id="more"></a><p>一般来说，在计算机视觉任务中，深度学习相关的迁移学习主要有两种类型:</p><ul><li>1.将网络当作特征提取器。</li><li>2.删除现有网络的全连接层，添加新的FC层，并微调这些权重识别新的类别数据。</li></ul><p><a href="http://lonepatient.top/2018/02/25/Deep_Learning_For_Computer_Vision_With_Python_PB_03.html">第3节</a>中，介绍了第一种方法。下面我们介绍另一种类型的迁移学习，如果有足够的数据，它实际上可以超越特征提取方法，这种方法称为微调，即我们利用新的数据对网络权重进行微调。首先，我们对预训练好的卷积神经网络（如VGG,ResNet或Inception等，一般是在大型数据集上训练得到的）的最后一组全连接层做截断处理(即删除网络中的top层)，然后，我们用一组新的全连接层与截断的已训练好的模型进行拼接，组成一个新的完整模型，并随机初始化权重[<strong>注意</strong>：这里的初始化权重只初始化新增的全连接层权重]，全连接层即Top部以下的所有层权重都被冻结，一般而言，训练时候是不更新，即后向传播是不在训练好的模型之间进行传播的。</p><p>在微调过程中，一般使用非常小的学习率来训练网络，这样新的全连接层就可以开始从已训练好的网络中学习新的模式。当然我们也可以对其他层的权重进行训练，这可能需要根据具体的数据来设置。利用微调技术，我们就不用重新开始训练模型，即可以节省大量的时间和精力，又可以获得更高的准确度。</p><p>在本章的其余部分，我们将更详细地讨论微调方法，以及如何对网络进行处理。最后，我们将应用于Flowers-17数据集中。</p><h2 id="迁移学习和微调"><a href="#迁移学习和微调" class="headerlink" title="迁移学习和微调"></a>迁移学习和微调</h2><p>微调方法是迁移学习的另一种类型。对于新数据集，我们首先加载预先训练好的模型，这时加载的模型一般并不适合于新数据，但是，该模型又具有良好的特征学习能力，想要保持该模型的强大区分能力，那怎么做呢？我们首先删除已有模型的全连接层，冻结剩余层的权重（包括在训练过程），然后构建一组新的全连接层，与已有的截断模型进行拼接，并对新的全连接层的权重进行初始化，最后，给定较小的学习率，将‘新’模型在新的数据上进行训练。通常，使用的预训练好的网络都是当前比较好的网络，如VGG、ResNet和Inception等，它们已经在大型数据集ImageNet上训练过。</p><p>正如我们在<a href="http://lonepatient.top/2018/02/25/Deep_Learning_For_Computer_Vision_With_Python_PB_03.html">第3节</a>讨论，这些预训练好的网络结构包含大量具有判别能力的过滤器，这些过滤器可用于新的数据集[虽然过滤器无法对新数据进行预测，但是可以提取有用的特征]。在这节，我们将对预训练好的网络结构进行修改，这样我们只需要训练修改的的部分网络参数，而不是重新开始训练整个网络。</p><p><a href="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/18-7-25/6867281.jpg" class="fancybox fancybox.image" rel="group"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/18-7-25/6867281.jpg" alt=""></a></p><center>图5.1 左：原始的VGG16网络结构， 右：网络结构的调整</center><p>我们以图5.1来理解微调是如何工作的。图5.1显示的是VGG16网络。从左图可以看到，最后一组(即“头”部分）主要是由全连接层和sofrmax分类器组成。使用微调方法时，我们对已有模型的网络结构进行修改，主要是删除头部，就像在<a href="http://lonepatient.top/2018/02/25/Deep_Learning_For_Computer_Vision_With_Python_PB_03.html">第3节</a>提到的提取特征一样操作。然而不同的是，微调过程中，我们<strong>实际上构建了一个新的由全连接层和softmax分类器组成的‘头’部，并与原始结构进行拼接，如图5.1右所示。</strong></p><p>在大多数情况下，新增的头部全连接层的参数会比原来的参数更少[当然实际上取决于你的特定数据集]。在大型数据上训练得到的模型往往具有强大的区分能力，而新增的全连接层是全新的且完全随机的。如果我们让这些随机值的梯度反向传播到整个网络，那么就可能破坏这些强大的特征。为了避开这个问题，我们仅训练头部全连接层参数，而“冻结”网络中其他层权重，如图5.2(左)所示。</p><p><a href="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/18-7-25/42336792.jpg" class="fancybox fancybox.image" rel="group"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/18-7-25/42336792.jpg" alt=""></a></p><center>图5.2 左：冻结所有conv层，微调全连接层， 右：全网络进行微调(调整全连接层之后）</center><p>微调训练过程中，前向传播是在整个网络之间进行传播，但是，后向传播只在新的全连接层之间进行，这样全连接层就可以从具有高区分能力的Conv层中学习特征。一般情况下，整个训练过程中，都不对全连接层之外的层进行训练。因为新的全连接层可能已经获得了相当好的准确度。但是，对于某些数据集，微调部分原始层参数可能会得到更好的效果，比如图5.2右。</p><p>当前面的FC层训练完之后，我们可以考虑微调部分原始网络的层权重，让原始网络也对新的数据进行学习。为了不让原始Conv层的过滤器发生明显的变化，设置非常小的学习速率，继续训练，直到获得足够好的准确度。</p><p>可见，我们可以利用预训练好的CNNs模型对自定义数据集经过微调得到新的图像分类器，在大多数情况下可以比提取特征方式获得更高的准确度。当然，微调也存在缺点，由于我们对网络结构进行了一定修改，如何确定需要修改的部分网络结构以及重新训练模型都是需要时间投入，从实践中得到一个相对好的“新”模型，且新的FC层参数的选择对网络精度存在重要影响。</p><p>其次，对于小数据集，从网络顶部就开始训练分类器可能不是最好的选择，这包含更多的数据集特定特征。另外，从网络前部的激活函数开始训练分类器可能更好一点。</p><h2 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h2><p>在对网络结构进行修改之前，我们需要了解网络的结构，即每一层对应的名字和位置。因为我们将根据网络层对应的索引对预训练好的模型某些层的权重进行“冻结”或“解冻”操作。</p><p>接下来，我们打印VGG16中的层名和索引。新建一个名为inspect_model.py文件，写入以下代码:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment">#encoding:utf-8</span></span><br><span class="line"><span class="comment"># 加载所需要模块</span></span><br><span class="line"><span class="keyword">from</span> keras.applications <span class="keyword">import</span> VGG16</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="comment"># 命令行参数设置</span></span><br><span class="line">ap = argparse.ArgumentParser()</span><br><span class="line">ap.add_argument(<span class="string">'-i'</span>,<span class="string">'--include_top'</span>,type = int,default=<span class="number">1</span>,help=<span class="string">'whether or not to include top of CNN'</span>)</span><br><span class="line">args = vars(ap.parse_args())</span><br><span class="line"><span class="comment"># 加载VGG16模型</span></span><br><span class="line">print(<span class="string">'[INFO] loading network...'</span>)</span><br><span class="line">model = VGG16(weights=<span class="string">'imagenet'</span>,include_top=args[<span class="string">'include_top'</span>]&gt;<span class="number">0</span>)</span><br><span class="line">print(<span class="string">"[INFO] showing layers..."</span>)</span><br><span class="line"><span class="comment"># 遍历VGG16所有层结构</span></span><br><span class="line"><span class="keyword">for</span> (i,layer) <span class="keyword">in</span> enumerate(model.layers):</span><br><span class="line">    print(<span class="string">"[INFO] {}\t{}"</span>.format(i,layer.__class__.__name__))</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>对于网络中的每一层，输出相应的索引i。根据这些信息，我们就会知道FC层从哪里开始。</p><p>执行下面命令，将在输出界面显示VGG16的网络结构:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="meta">$</span><span class="bash"> python inspect_model.py</span></span><br></pre></td></tr></tbody></table></figure></div><p></p><p>结果如下:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line">[INFO] showing layers...</span><br><span class="line">[INFO] 0	InputLayer</span><br><span class="line">[INFO] 1	Conv2D</span><br><span class="line">[INFO] 2	Conv2D</span><br><span class="line">[INFO] 3	MaxPooling2D</span><br><span class="line">[INFO] 4	Conv2D</span><br><span class="line">[INFO] 5	Conv2D</span><br><span class="line">[INFO] 6	MaxPooling2D</span><br><span class="line">[INFO] 7	Conv2D</span><br><span class="line">[INFO] 8	Conv2D</span><br><span class="line">[INFO] 9	Conv2D</span><br><span class="line">[INFO] 10	MaxPooling2D</span><br><span class="line">[INFO] 11	Conv2D</span><br><span class="line">[INFO] 12	Conv2D</span><br><span class="line">[INFO] 13	Conv2D</span><br><span class="line">[INFO] 14	MaxPooling2D</span><br><span class="line">[INFO] 15	Conv2D</span><br><span class="line">[INFO] 16	Conv2D</span><br><span class="line">[INFO] 17	Conv2D</span><br><span class="line">[INFO] 18	MaxPooling2D</span><br><span class="line">[INFO] 19	Flatten</span><br><span class="line">[INFO] 20	Dense</span><br><span class="line">[INFO] 21	Dense</span><br><span class="line">[INFO] 22	Dense</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>从结果中可以看到，20-22层是全连接层。接下来，将—include_top设置为-1，即不返回FC层:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="meta">$</span><span class="bash"> python inspact_model.py --include_top -1</span></span><br></pre></td></tr></tbody></table></figure></div><p></p><p>得到的结果如下:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line">[INFO] showing layers...</span><br><span class="line">[INFO] 0	InputLayer</span><br><span class="line">[INFO] 1	Conv2D</span><br><span class="line">[INFO] 2	Conv2D</span><br><span class="line">[INFO] 3	MaxPooling2D</span><br><span class="line">[INFO] 4	Conv2D</span><br><span class="line">[INFO] 5	Conv2D</span><br><span class="line">[INFO] 6	MaxPooling2D</span><br><span class="line">[INFO] 7	Conv2D</span><br><span class="line">[INFO] 8	Conv2D</span><br><span class="line">[INFO] 9	Conv2D</span><br><span class="line">[INFO] 10	MaxPooling2D</span><br><span class="line">[INFO] 11	Conv2D</span><br><span class="line">[INFO] 12	Conv2D</span><br><span class="line">[INFO] 13	Conv2D</span><br><span class="line">[INFO] 14	MaxPooling2D</span><br><span class="line">[INFO] 15	Conv2D</span><br><span class="line">[INFO] 16	Conv2D</span><br><span class="line">[INFO] 17	Conv2D</span><br><span class="line">[INFO] 18	MaxPooling2D</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>可以看到，网络最后一层为Pool层。我们后面将使用上面的模型结构。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>在替换预训练好的模型”头”部分之前，我们先定义一个新的‘头’。创建一个名为fcheadnet.py文件，放入pyimagesearch项目中的nn.conv子模块中，结构如下所示：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line">--- pyimagesearch</span><br><span class="line">|    |--- __init__.py</span><br><span class="line">|    |--- callbacks</span><br><span class="line">|    |--- io</span><br><span class="line">|    |--- nn</span><br><span class="line">|        |--- __init__.py</span><br><span class="line">|        |--- conv</span><br><span class="line">|            |--- __init__.py</span><br><span class="line">|            |--- lenet.py</span><br><span class="line">|            |--- minivggnet.py</span><br><span class="line">|            |--- fcheadnet.py</span><br><span class="line">|            |--- shallownet.py</span><br><span class="line">|    |--- preprocessing</span><br><span class="line">|    --- utils</span><br></pre></td></tr></tbody></table></figure></div><p>并写入以下代码:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment">#encoding:utf-8</span></span><br><span class="line"><span class="comment"># 加载所需要的模块</span></span><br><span class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Dropout</span><br><span class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Flatten</span><br><span class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Dense</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>下面，定义FCHeadNet 类:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FCHeadNet</span>:</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(baseModel,classes,D)</span>:</span></span><br><span class="line">        <span class="comment"># 初始化top部分</span></span><br><span class="line">        headModel = baseModel.output</span><br><span class="line">        headModel = Flatten(name=<span class="string">'flatten'</span>)(headModel)</span><br><span class="line">        headModel = Dense(D,activation=<span class="string">'relu'</span>)(headModel)</span><br><span class="line">        headModel = Dropout(<span class="number">0.5</span>)(headModel)</span><br><span class="line">        <span class="comment"># 增加一个softmaxc层</span></span><br><span class="line">        headModel = Dense(classes,activation=<span class="string">'softmax'</span>)(headModel)</span><br><span class="line">        <span class="keyword">return</span> headModel</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>其中：</p><ul><li>baseModel：网络的主体，比如上面的VGG16（只到18层）</li><li>classes：数据集中的类别个数，比如Flowers-17的类别个数为17</li><li>D：全连接层的节点数</li></ul><p>新构建的网络结构如下：<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line">INPUT =&gt; FC =&gt; RELU =&gt; DO =&gt; FC =&gt; SOFTMAX</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>新的全连接层比原始的VGG16的全连接层相对要简单点，原始的VGG16包含两组4096个节点的FC层。大多数我们进行微调操作时，并不是要复制原始网络的”头”结构，而是要简化它，以便更容易进行微调——头中的参数越少，我们就越有可能正确地将网络调优到适合新的分类任务。</p><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>新建一个名为finetune_flowers17.py的文件，并写入以下代码:</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment">#encoding:utf-8</span></span><br><span class="line"><span class="comment"># 加载所需要模块</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelBinarizer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"><span class="comment"># 数据预处理</span></span><br><span class="line"><span class="keyword">from</span> pyimagesearch.preprocessing <span class="keyword">import</span> ImageToArrayPreprocessor <span class="keyword">as</span> ITAP</span><br><span class="line"><span class="keyword">from</span> pyimagesearch.preprocessing <span class="keyword">import</span> AspectAwarePreprocessor <span class="keyword">as</span> AAP</span><br><span class="line"><span class="keyword">from</span> pyimagesearch.datasets <span class="keyword">import</span> SimpleDatasetLoader <span class="keyword">as</span> SDL</span><br><span class="line"><span class="keyword">from</span> pyimagesearch.nn.conv <span class="keyword">import</span> fcheadnet <span class="keyword">as</span> FCN <span class="comment"># 新的全连接层</span></span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.image   <span class="keyword">import</span> ImageDataGenerator <span class="comment"># 数据增强</span></span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> RMSprop</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> keras .applications <span class="keyword">import</span> VGG16</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> SGD</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> imutils <span class="keyword">import</span> paths</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> os</span><br></pre></td></tr></tbody></table></figure></div><p>定义命令行参数:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 解析命令行参数</span></span><br><span class="line">ap = argparse.ArgumentParser()</span><br><span class="line">ap.add_argument(<span class="string">"-d"</span>,<span class="string">"--dataset"</span>,required=<span class="keyword">True</span>,help=<span class="string">'path to input dataset'</span>)</span><br><span class="line">ap.add_argument(<span class="string">'-m'</span>,<span class="string">'--model'</span>,required=<span class="keyword">True</span>,help=<span class="string">'path to output model'</span>)</span><br><span class="line">args = vars(ap.parse_args())</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>其中；</p><ul><li>—dataset：输入数据的目录路径</li><li>—model：模型的保存路径</li></ul><p>同样，我们对train数据进行数据增强操作：<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 数据增强</span></span><br><span class="line">aug = ImageDataGenerator(rotation_range=<span class="number">30</span>,width_shift_range=<span class="number">0.1</span>,height_shift_range=<span class="number">0.1</span>,shear_range=<span class="number">0.2</span>,zoom_range=<span class="number">0.2</span>,horizontal_flip=<span class="keyword">True</span>,fill_mode=<span class="string">'nearest'</span>)</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>正如在<a href="http://lonepatient.top/2018/02/18/Deep_Learning_For_Computer_Vision_With_Python_PB_02.html">第2节</a>中提到的，大多数情况我们都应该应用数据增强，因为数据增强既可以提高模型的准确度又可以避免过拟合，而且当我们没有足够的数据从头开始训练一个CNN模型时，我们更应该使用数据增强。</p><p>下面，我们从磁盘中加载数据集，并对数据进行处理：<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 从磁盘中加载图片，并提取标签</span></span><br><span class="line">print(<span class="string">"[INFO] loading images..."</span>)</span><br><span class="line">imagePaths = list(paths.list_images(args[<span class="string">'dataset'</span>]))</span><br><span class="line">classNames = [pt.split(os.path.sep)[<span class="number">-2</span>] <span class="keyword">for</span> pt <span class="keyword">in</span> imagePaths]</span><br><span class="line">classNames = [str(x) <span class="keyword">for</span> x <span class="keyword">in</span> np.unique(classNames)]</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>需要注意的下，数据结构目录服从下面格式：<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line">dataset_name/{class_name}/example.jpg</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>这样的好处就是可以方便地提取数据的类别信息。</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 初始化图像预处理</span></span><br><span class="line">aap = AAP.AspectAwarePreprocesser(<span class="number">224</span>,<span class="number">224</span>)</span><br><span class="line">iap= ITAP.ImageToArrayPreprocess()</span><br><span class="line"><span class="comment"># 加载图像数据，并进行图像数据预处理</span></span><br><span class="line">sdl = SDL.SimpleDatasetLoader(preprocessors=[aap,iap])</span><br><span class="line">(data,labels)  = sdl.load(imagePaths,verbose=<span class="number">500</span>)</span><br><span class="line">data = data.astype(<span class="string">"float"</span>) / <span class="number">255.0</span></span><br></pre></td></tr></tbody></table></figure></div><p>数据划分以及标签编码处理:<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 数据划分</span></span><br><span class="line">(trainX,testX,trainY,testY) = train_test_split(data,labels,test_size=<span class="number">0.25</span>,random_state=<span class="number">42</span>)</span><br><span class="line"><span class="comment"># 标签进行编码化处理</span></span><br><span class="line">trainY = LabelBinarizer().fit_transform(trainY)</span><br><span class="line">testY = LabelBinarizer().fit_transform(testY)</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>接下来，我们开始构建“新”的模型。<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 加载VGG16网络，不返回原始模型的全连接层</span></span><br><span class="line">baseModel = VGG16(weights=<span class="string">'imagenet'</span>,include_top=<span class="keyword">False</span>,input_tensor=Input(shape = (<span class="number">224</span>,<span class="number">224</span>,<span class="number">3</span>)))</span><br><span class="line"><span class="comment"># 初始化新的全连接层</span></span><br><span class="line">headModel = FCN.FCHeadNet.build(baseModel,len(classNames),<span class="number">256</span>)</span><br><span class="line"><span class="comment"># 拼接模型</span></span><br><span class="line">model = Model(inputs=baseModel.input,outputs = headModel)</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>首先，我们加载了预训练好的VGG16模型，不返回全连接层[即删除‘头’部分]，然后，加载自定义的全连接层，最后，将两部分进行拼接，组成一个“新”的模型。</p><p>前面提到过，在训练之前，我们需要“冻结”已有模型的权重，这样它们在反向传播阶段就不会被更新。keras中很容易实现‘冻结’操作，我们通过对baseModel中每个层设置.trainable参数为False来实现：<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 遍历所有层，并冻结对应层的权重</span></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> baseModel.layers:</span><br><span class="line">    layer.trainable = <span class="keyword">False</span></span><br></pre></td></tr></tbody></table></figure></div><p></p><p>接着，我们开始初始化全连接层的权重以及进行训练：<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line">print(<span class="string">"[INFO] compiling model..."</span>)</span><br><span class="line">opt = RMSprop(lr=<span class="number">0.001</span>)</span><br><span class="line">model.compile(loss=<span class="string">"categorical_crossentropy"</span>, optimizer=opt,metrics=[<span class="string">"accuracy"</span>])</span><br><span class="line"><span class="comment"># 由于我们只训练新增的全连接层，因此，我们进行少量迭代</span></span><br><span class="line">print(<span class="string">"[INFO] training head..."</span>)</span><br><span class="line">model.fit_generator(aug.flow(trainX,trainY,batch_size = <span class="number">32</span>),</span><br><span class="line">                             validation_data = (testX,testY),epochs=<span class="number">25</span>,</span><br><span class="line">                             steps_per_epoch = len(trainX) //<span class="number">32</span>,verbose = <span class="number">1</span>)</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>这里使用的是RMSprop优化器，我们将在<a href="">第7节</a>中详细讨论这个算法。前面提到，一般微调过程使用一个很小的学习率，因此，设置lr=0.001。上面部分，我们主要的目的是训练“头”部分权重，而不改变网络主体的权重，因此，epochs设置为25，当然可以根据你的具体数据集进行调整，一般设置在10-30epochs左右。</p><p>查看模型的性能结果：<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 评估模型</span></span><br><span class="line">print(<span class="string">"[INFO] evaluating after initialization..."</span>)</span><br><span class="line">predictions = model.predict(testX,batch_size=<span class="number">32</span>)</span><br><span class="line">print(classification_report(testY.argmax(axis =<span class="number">1</span>),</span><br><span class="line">                            predictions.argmax(axis =<span class="number">1</span>),target_names=classNames))</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>完成了全连接层的训练之后，接下来，我们微调原有模型的部分权重，同样，通过对baseModel中每个层设置.trainable参数为True来实现<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 对整个网络进行微调</span></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> baseModel.layers[<span class="number">15</span>:]:</span><br><span class="line">    layer.trainable = <span class="keyword">True</span></span><br></pre></td></tr></tbody></table></figure></div><p></p><p>对于很深的网络比如VGG，包含许多参数，建议只微调部分层的权重，当然如果模型的准确度有提高且没有发生过拟合问题，那么可以微调更多层的权重。</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 从新编译模型</span></span><br><span class="line">print(<span class="string">"[INFO] re-compiling model ..."</span>)</span><br><span class="line">opt = SGD(lr=<span class="number">0.001</span>)</span><br><span class="line"><span class="comment"># 使用很小的学习率进行微调</span></span><br><span class="line">model.compile(loss = <span class="string">'categoricla_crossentropy'</span>,optimizer = opt,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"><span class="comment"># 对整个模型进行微调</span></span><br><span class="line">print(<span class="string">"[INFO] fine-tuning model..."</span>)</span><br><span class="line">model.fit_generator(aug.flow(trainX,trainY,batch_size=<span class="number">32</span>),</span><br><span class="line">                    validation_data = (testX,testY),epochs = <span class="number">100</span>,</span><br><span class="line">                    steps_per_epoch = len(trainX) // <span class="number">32</span>,verbose = <span class="number">1</span>)</span><br></pre></td></tr></tbody></table></figure></div><p>保存模型权重：<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="comment"># 评估微调后的模型结果</span></span><br><span class="line">print(<span class="string">"[INFO] evaluating after fine-tuning..."</span>)</span><br><span class="line">predictions = model.predict(testX,batch_size=<span class="number">32</span>)</span><br><span class="line">print(classification_report(testY.argmax(axis =<span class="number">1</span>),</span><br><span class="line">        predictions.argmax(axis =<span class="number">1</span>),target_names=classNames))</span><br><span class="line"><span class="comment"># 将模型保存到磁盘</span></span><br><span class="line">print(<span class="string">"[INFO] serializing model..."</span>)</span><br><span class="line">model.save(args[<span class="string">'model'</span>])</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>接下来，在Flower-17数据集进行实验，<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line"><span class="meta">$</span><span class="bash"> python finetune_flowers17.py --dataset yourPath/datasets/flowers17/images --model flowers17.model</span></span><br></pre></td></tr></tbody></table></figure></div><p></p><p>将得到以下结果；<br></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line">[INFO] loading images...</span><br><span class="line">[INFO] processed 500/1360</span><br><span class="line">[INFO] processed 1000/1360</span><br><span class="line">[INFO] compiling model...</span><br><span class="line">[INFO] training head...</span><br><span class="line">Epoch 1/25</span><br><span class="line">10s - loss: 4.8957 - acc: 0.1510 - val_loss: 2.1650 - val_acc: 0.3618</span><br><span class="line">...</span><br><span class="line">Epoch 10/25</span><br><span class="line">10s - loss: 1.1318 - acc: 0.6245 - val_loss: 0.5132 - val_acc: 0.8441</span><br><span class="line">...</span><br><span class="line">Epoch 23/25</span><br><span class="line">10s - loss: 0.7203 - acc: 0.7598 - val_loss: 0.4679 - val_acc: 0.8529</span><br><span class="line">Epoch 24/25</span><br><span class="line">10s - loss: 0.7355 - acc: 0.7520 - val_loss: 0.4268 - val_acc: 0.8853</span><br><span class="line">Epoch 25/25</span><br><span class="line">10s - loss: 0.7504 - acc: 0.7598 - val_loss: 0.3981 - val_acc: 0.8971</span><br><span class="line">[INFO] evaluating after initialization...</span><br><span class="line">precision recall f1-score support</span><br><span class="line">  bluebell 0.75 1.00 0.86 18</span><br><span class="line"> buttercup 0.94 0.85 0.89 20</span><br><span class="line"> coltsfoot 0.94 0.85 0.89 20</span><br><span class="line">   cowslip 0.70 0.78 0.74 18</span><br><span class="line">    crocus 1.00 0.80 0.89 20</span><br><span class="line">  daffodil 0.87 0.96 0.91 27</span><br><span class="line">     daisy 0.90 0.95 0.93 20</span><br><span class="line"> dandelion 0.96 0.96 0.96 23</span><br><span class="line">fritillary 1.00 0.86 0.93 22</span><br><span class="line">      iris 1.00 0.95 0.98 21</span><br><span class="line">lilyvalley 0.93 0.93 0.93 15</span><br><span class="line">     pansy 0.83 1.00 0.90 19</span><br><span class="line">  snowdrop 0.88 0.96 0.92 23</span><br><span class="line"> sunflower 1.00 0.96 0.98 23</span><br><span class="line"> tigerlily 0.90 1.00 0.95 19</span><br><span class="line">     tulip 0.86 0.38 0.52 16</span><br><span class="line">windflower 0.83 0.94 0.88 16</span><br><span class="line">avg /total 0.90 0.90 0.89 340</span><br></pre></td></tr></tbody></table></figure></div><p></p><p>在第一个epoch，验证集的准确度很低(约36%)，主要一开始新的全连接层的权重是随机初始化的。随着优化不断进行，准确率迅速上升——到第10个epoch时，我们的分类准确率超过了80%，到第25个epoch结束时，准确率几乎达到了90%。</p><p>仅仅微调全连层，我们的新模型达到了约90%的准确度。接下来，我们查看微调部分原始模型的权重的结果，如下所示：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><button class="btn-copy" data-clipboard-snippet="">  <i class="fa fa-clipboard"></i><span>copy</span></button><pre><span class="line">...</span><br><span class="line">[INFO] re-compiling model...</span><br><span class="line">[INFO] fine-tuning model...</span><br><span class="line">Epoch 1/100</span><br><span class="line">12s - loss: 0.5127 - acc: 0.8147 - val_loss:0.3640 - val_acc: 0.8912</span><br><span class="line">...</span><br><span class="line">Epoch 99/100</span><br><span class="line">12s - loss: 0.1746 - acc: 0.9373 - val_loss:0.2286 - val_acc: 0.9265</span><br><span class="line">Epoch 100/100</span><br><span class="line">12s - loss: 0.1845 - acc: 0.9402 - val_loss:0.2019 - val_acc: 0.9412</span><br><span class="line">[INFO] evaluating after fine-tuning...</span><br><span class="line">             precision    recall  f1-score   support</span><br><span class="line">          0       0.94      0.79      0.86        19</span><br><span class="line">          1       0.93      0.87      0.90        15</span><br><span class="line">         10       1.00      1.00      1.00        20</span><br><span class="line">         11       0.95      0.83      0.88        23</span><br><span class="line">         12       0.95      0.95      0.95        19</span><br><span class="line">         13       0.82      0.86      0.84        21</span><br><span class="line">         14       0.95      0.95      0.95        20</span><br><span class="line">         15       1.00      0.93      0.96        27</span><br><span class="line">         16       0.89      1.00      0.94        16</span><br><span class="line">          2       0.90      0.95      0.93        20</span><br><span class="line">          3       0.90      0.90      0.90        20</span><br><span class="line">          4       1.00      0.95      0.98        22</span><br><span class="line">          5       1.00      1.00      1.00        16</span><br><span class="line">          6       0.90      1.00      0.95        18</span><br><span class="line">          7       0.77      0.94      0.85        18</span><br><span class="line">          8       0.92      0.96      0.94        23</span><br><span class="line">          9       1.00      0.96      0.98        23</span><br><span class="line">avg / total       0.93      0.93      0.93       340</span><br></pre></td></tr></tbody></table></figure></div><p>从结果中可以看到，准确度提高到了93%，比我们之前提取特征的方法更高。</p><p>从实验的结果来看，微调方法可以比特征提取方法获得更高的准确度。它能够将原始的网络权重对新的数据集进行学习——这是特征提取所不允许的。因此，当给定足够的训练数据时，尽量使用微调，因为你可能会比仅使用简单的特征提取获得更高的分类准确度。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在本章中，我们讨论了迁移学习的另一种类型——微调。加载训练好的模型（一般实在大型数据集上训练得到的），截取全连接层以下的层作为“新”模型的主体，并在主体上连接新的全连接层。训练过程中，我们冻结主体层的权重，只训练新的全连接层参数。当然我们也可以微调主体的部分层权重。利用微调技术，因为我们不必从头训练整个网络。相反，我们可以利用已经存在的网络架构，例如在ImageNet数据集上训练的最优秀的模型。</p><p>本章完整代码地址:<a href="https://github.com/lonePatient/Deep_Learning_For_Computer_Vision_With_Python/tree/master/PB" target="_blank" rel="noopener">github</a></p></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/深度学习/" rel="tag"><i class="fa fa-envira"></i> 深度学习</a> <a href="/tags/计算机视觉/" rel="tag"><i class="fa fa-envira"></i> 计算机视觉</a></div><div class="post-widgets"><div id="needsharebutton-postbottom"><span class="btn"><i class="fa fa-share-alt" aria-hidden="true"></i></span></div></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2018/03/11/understand LSTM.html" rel="prev" title="理解LSTM"><i class="fa fa-chevron-left"></i> 理解LSTM</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2018/03/04/evaluate-your-machine-learning-algorithm.html" rel="next" title="常用的机器学习算法衡量指标">常用的机器学习算法衡量指标 <i class="fa fa-chevron-right"></i></a></div></div></footer></div>
https://lonepatient.top/2018/03/10/Deep_Learning_For_Computer_Vision_With_Python_PB_05.html
